{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text by the Numbers: Counting Words across Multiple Texts\n",
    "\n",
    "**A Reproducible Research Workshop**\n",
    "\n",
    "(A Collaboration between Dartmouth Library and Research Computing)\n",
    "\n",
    "[*Click here to view or register for our current list of workshops*](http://dartgo.org/RRADworkshops)\n",
    "\n",
    "*This notebook created by*:\n",
    "+ Version 1.0: Jeremy Mikecz, Research Data Services (Dartmouth Library)\n",
    "\n",
    "<!--\n",
    "+ Some of the inspiration for the code and information in this notebook was taken from https://www.w3schools.com/python/python_intro.asp -- This is a great resource if you want to learn more about Python!-->\n",
    "\n",
    "**How can we use computational techniques to analyze texts and then visualize patterns buried within them?** \n",
    "\n",
    "**What can we learn about texts by applying text analysis in Python? How do we get started?**\n",
    "\n",
    "This is **Notebook 2** of 3 for the **Text by the Numbers: Text Analysis Methods in Python** workshop:\n",
    "+ Notebook 1: Text by the Numbers: Working with one text\n",
    "+ **Notebook 2: Text by the Numbers 2: Working with a Corpus**\n",
    "+ Notebook 3: Text by the Numbers 3: Word Vectors\n",
    "\n",
    "In this session, participants will:\n",
    "\n",
    "+ Learn how to write basic scripts in Python using Jupyter Notebooks\n",
    "+ import and pre-process documents (into lists of words or tokens, lower-case each word, remove \"stopwords\", etc.)\n",
    "+ analyze each document using word frequencies, collocations, ngram frequencies, etc.\n",
    "\n",
    "**Table of Contents**\n",
    "\n",
    "+ I. Setup\n",
    "+ II. Create a dataframe from a corpus\n",
    "+ III. Create an exploratory visualization of the corpus\n",
    "+ IV. Visualizing the frequency of words across a corpus\n",
    "+ V. Reproducible visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part I: Setup\n",
    "\n",
    "1. Before beginning, we need to import some packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "from pathlib import Path\n",
    "import glob \n",
    "\n",
    "textdir = Path(\"~/shared/RR-workshop-data/state-of-the-union-dataset/txt\").expanduser() \n",
    "pathlist = sorted(textdir.glob('*.txt')) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "    <p style=\"color:blue\"><b>Exercises</b>:</p> \n",
    "    <p style=\"color:blue\">2. Choose a corpus you would like to analyze further. Save a Path to this corpus. You can see if it worked correctly by running: `[path.name for path in pathlist]`\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Create a Dataframe of processed texts\n",
    "\n",
    "**3. DATAFRAMES:** To enable easier analysis of a corpus of texts, we can store info about each in a **dataframe**. A dataframe in Python is a common data structure enabled with the **pandas** library. It is a two-dimensional data table that stores data in rows and columns. \n",
    "\n",
    "Run the code below, and then examine what each portion of the code does (see the \"#comments\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk import RegexpTokenizer  \n",
    "from nltk.corpus import stopwords\n",
    "stop = sorted(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "#n=50\n",
    "\n",
    "txtList=[]\n",
    "pathlist = sorted(textdir.glob('*.txt'))      # .glob only stores the pathlist temporarily (for some reason), so you need to call it again!2\n",
    "for path in pathlist:\n",
    "    fn=path.stem                       #stem returns the filename minus the \".txt\" (file extension). \n",
    "    pres,year=fn.split(\"_\")            # fn = \"1794_Washington\" becomes year = \"1794\" and pres = \"Washington\"\n",
    "    with open(path,'r') as f:  \n",
    "        text1 = f.read()                #opens each file and reads it in as \"sotu\"\n",
    "    tokens=tokenizer.tokenize(text1)    # tokenizes \"sotu\"\n",
    "    numtoks = len(tokens)             # counts the number of tokens in \"sotu\"\n",
    "    ltokens_ns = [tok.lower() for tok in tokens if tok not in stop]\n",
    "    txtList.append([pres, year, numtoks, tokens, ltokens_ns, text1])   #add this info for \"sotu\" to a running list for all sotu addresses\n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. We have thus stored information about each text into a list call `txtList`. Now we want to read in this list of information into a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colnames=['pres','year','numtoks','tokens', 'ltoks_ns', 'fulltext']\n",
    "textdf=pd.DataFrame(txtList, columns=colnames)  #places our completed list of SOTU info in a dataframe\n",
    "textdf.head(10)                                #prints out the first 10 rows of this dataframe (the default value for head() is 5 rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "    <p style=\"color:blue\"><b>Exercises</b>:</p> \n",
    "    <p style=\"color:blue\">5. Using your chosen corpus:</p>\n",
    "    <ol style=\"color:blue\"> \n",
    "        <li>read it</li> \n",
    "        <li>lower-case it</li> \n",
    "        <li>remove all stopwords</li>\n",
    "        <li>count the number of tokens</li>\n",
    "        <li>and read this information into a dataframe</li>\n",
    "    </ol>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III. Create an exploratory visualization of the corpus\n",
    "\n",
    "6. Now that we have placed our State of the Union (SOTU) corpus into a dataframe, we can create some basic visualizations. For example, we can create a chart comparing the changing length of SOTU addresses over time.\n",
    "\n",
    "Fortunately, Python's **seaborn** library makes the creation of visualizations really easy. In the code below, we import seaborn (with its conventional abbreviation of \"sns\") and **matplotlib** the visualization library on which seaborn is built. Then, in one line of code, we can create a bar graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.barplot(textdf, x = \"year\", y = \"numtoks\", hue = \"pres\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. But, as you notice, creating a visualization may be easy, but creating one that is legible and interpretable often requires more work. \n",
    "\n",
    "What problems do you notice in the graph above? How would you go about correcting them?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IV. Visualizing the frequency of words across a corpus\n",
    "\n",
    "8. One of the most basic forms of text analysis is to calculate and visualize the frequency of particular words or terms (ngrams) across a corpus. We are going to do such a thing here. But, first we should re-examine our dataset:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "textdf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. To identify the frequency of a given term for a given speech, we will want to create a frequency list of words (as found in out lower-case tokens column) and then identify the frequency in which a chose word is found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections    # use for creating frequency lists\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_nums = [3, 5, 7, 1, 4, 2, 12, 1, 28, 4, 5, 57, 1, 54, -4, 6, 2, 1, 2, 7, 5, 3, 88, 99, 57, 36, 1, 4, 2, 3, 8, 9, 15, 2, 3, 7]\n",
    "collections.Counter(list_of_nums)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. We can retrieve the frequency of a particular number by 1) saving the Counter object (essentially frequency list stored as a dictionary) into memory, and 2) retrieving the count for a particular item."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numfreq = collections.Counter(list_of_nums)\n",
    "numfreq[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11. We can do the same to a list of (lower-case) tokens..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ltoks_sample = textdf.loc[0, \"ltoks_ns\"]\n",
    "ltoks_sample[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11b. create a function to retrieve the frequency of a word in a list of tokens..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getWordFreq (ltoks, term):\n",
    "    tokfreqs=collections.Counter(ltoks)\n",
    "    wordFreq = tokfreqs[term]\n",
    "    return(wordFreq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11c. call that function on our sample list of tokens and with a given search word..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getWordFreq(ltoks_sample, \"war\")   #try replacing \"war\" with a word of your choice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11d. ... and then use the dataframe **.apply()** method to apply this function on our lower-case tokens column and create a new column that reports the number of times a given search term appears in that list of tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_term = \"freedom\"\n",
    "textdf['wordfreq'] = textdf['ltoks_ns'].apply(getWordFreq, term = search_term)\n",
    "textdf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12. But, to better fit this information on a graph, we can aggregate the data by president to reduce our dataset from 233 observations (speeches) to 44 (presidents).\\* This means, for example, that Ronald Reagan's 8 SOTU speeches are compiled into one combined text. To do this, first we need to sort the dataframe by the year of the address.\n",
    "\n",
    "\\* Yes, Biden is considered the 46th president. But, two presidents died without ever giving a SOTU speech, hence the number 44. Even that number is debatable as Grover Cleveland is considered the US's 22nd and 24th president as he served two non-consecutive terms. And, then there is the matter of the various presidents of the Congress of the Confederation that served from 1774 to 1788."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "textdf = textdf.sort_values(by = \"year\")\n",
    "textdf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12b. and then assign a number to each new administration. Since our dataframe is now sorted by year, we can use the **.shift()** method to create a new colum for presidential number (\"presnum\") that increases by one every time a new president (\"pres\") appears in our sorted dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "textdf[\"presnum\"] = (textdf[\"pres\"] != textdf[\"pres\"].shift()).cumsum()\n",
    "textdf.tail(12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12c. We can then group the dataframe by president number. We can use the the **.agg()** method to choose how to aggregate the data in the columns we which to keep. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "textdf2 = textdf.groupby(['presnum']).agg({'pres':'first','wordfreq':'sum','numtoks':'sum','year':'first'})\n",
    "textdf2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "13. However, as we noticed in the bar plot in #6 above, the length of SOTU speeches has varied greatly over time, from addresses that barely topped 1,000 words to those that nearly reached 40,000. \n",
    "\n",
    "Thus, for better comparison, we will want to calculate the frequency rather than just absolute number of times a word appears. This can be done by dividing the absolute word frequency of a search term by the number of words (ahem... tokens). We can then multiply the result by 1,000,000 to get the frequency a term was used across one million words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "textdf2['freq_perMillion'] = textdf2['wordfreq'] / textdf2['numtoks'] * 1000000\n",
    "textdf2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "textdf2.sort_values('freq_perMillion',ascending=False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "14. We can then graph the results, but this time tweaking some settings in matplotlib and seaborn to create a chart that is more legible:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = [12, 8]  #changes default figure size to make larger plots\n",
    "%config InteractiveShellApp.matplotlib = 'inline'\n",
    "%config InlineBackend.figure_formats = ['svg']\n",
    "\n",
    "g=sns.barplot(data=textdf2, x=\"year\", y=\"freq_perMillion\", hue = \"pres\")\n",
    "g.tick_params(labelrotation=90)\n",
    "g.set(title = \"Frequency of '%s' in State of the Union Addresses\"%search_term)\n",
    "g.set(ylabel='per million words', xlabel='President')\n",
    "g.set(xticklabels = textdf2.pres)\n",
    "plt.legend([],[], frameon=False); #adding the \";\" removes the annoying text that Python sometimes prints out with a graphic#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V. Reproducible visualization\n",
    "\n",
    "15. This seems to have worked well. But, now imagine that we want to re-run this code multiple times to explore the frequency of different words. To enable such repetition, it helps to place our relevant code inside functions that can easily be called:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_search2(ltoksCol,searchTerm): #returns a column of frequencies after searching for a term across a column of lower-case tokens\n",
    "    #searchTerm = searchTerm.lower()\n",
    "    wordfreq = ltoksCol.apply(lambda x:collections.Counter(x)[searchTerm])\n",
    "    return(wordfreq)\n",
    "\n",
    "def df_wordfreqcalc(old_df,searchTerm):   \n",
    "    \"\"\"\n",
    "    reads in a dataframe of SOTU addresses by year and a searchTerm \n",
    "    returns a dataframe aggregated by President, with the 'wordfreq' and 'freq_perMillion' calculated for each president\n",
    "       which is calculated using the sotuWordSearch2 function\n",
    "    \"\"\"\n",
    "    requiredCols = ['ltoks','numtoks','pres','presnum','year']\n",
    "    if not set(requiredCols).issubset(old_df.columns):\n",
    "        print(\"missing required column from:\",requiredCols)\n",
    "        return None\n",
    "    old_df['wordfreq'] = word_search2(old_df['ltoks'],searchTerm) \n",
    "    new_df = old_df.groupby(['pres','presnum']).agg({'wordfreq':'sum','numtoks':'sum','year':'first'})\n",
    "    #print(new_df.head(2))\n",
    "    new_df['freq_perMillion'] = new_df['wordfreq'] / new_df['numtoks'] * 1000000\n",
    "    new_df = new_df.sort_values(['year'])\n",
    "    new_df = new_df.reset_index()\n",
    "    print(\"searching for... :\",searchTerm)\n",
    "    return(new_df)\n",
    " \n",
    "def create_wordfreqplot(df,searchTerm):            \n",
    "    \"\"\"\n",
    "    #reads in our aggregated SOTU dataframe and creates a bar plot of the search term\n",
    "    \"\"\"\n",
    "    #newdf = df_wordFreqCalc(df,searchTerm)\n",
    "    requiredCols = ['freq_perMillion','pres','presnum']\n",
    "    if not set(requiredCols).issubset(df.columns):\n",
    "        print(\"missing required column from:\",requiredCols)\n",
    "        return None\n",
    "    g=sns.barplot(data=df, x=\"presnum\",y=\"freq_perMillion\", hue = \"pres\")\n",
    "    g.tick_params(labelrotation=90)\n",
    "    g.set(title = \"Frequency of '%s' in State of the Union Addresses\"%searchTerm)\n",
    "    g.set(ylabel='per million words', xlabel='President')\n",
    "    g.set(xticklabels = df.pres)\n",
    "    plt.legend([],[], frameon=False);\n",
    "    return(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "textdf = textdf.rename(columns = {\"ltoks_ns\": \"ltoks\"})\n",
    "textdf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "16. Now, to create graphs charting the frequency of different terms, all we need to run are the three lines of code below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "searchTerm = \"indian\"      # try replacing this with other terms (but remember this is searching on a lower-case list of tokens, so you will want to input a lower-case word)\n",
    "textdf3 = df_wordfreqcalc(textdf,searchTerm)\n",
    "create_wordfreqplot(textdf3,searchTerm);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "    <p style=\"color:blue\"><b>Exercises</b>:</p> \n",
    "    <p style=\"color:blue\">17. Using the custom functions above and the SOTU dataframe:</p>\n",
    "    <ol style=\"color:blue\"> \n",
    "        <li>a. Choose a search term</li>\n",
    "        <li>1. Create some bar graphs showing the distribution of this term across the corpus.</li> \n",
    "    </ol>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "    <p style=\"color:blue\"><b>Exercises - Advanced</b>:</p> \n",
    "    <p style=\"color:blue\">18. Create a series of functions that re-create the frequency graphs above but for a different corpus. Note: the functions above were specifically designed with the SOTU dataset in mind and require columns such as \"pres\", \"presnum\", and \"year.\" How will you go about adjusting those functions (or starting from scratch) in a way that works with different types of corpora?</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
