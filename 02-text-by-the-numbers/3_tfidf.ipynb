{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text by the Numbers: Word Vectors\n",
    "\n",
    "**A Reproducible Research Workshop**\n",
    "\n",
    "(A Collaboration between Dartmouth Library and Research Computing)\n",
    "\n",
    "[*Click here to view or register for our current list of workshops*](http://dartgo.org/RRADworkshops)\n",
    "\n",
    "*This notebook created by*:\n",
    "+ Version 1.0: Jeremy Mikecz, Research Data Services (Dartmouth Library)\n",
    "<!--\n",
    "+ Some of the inspiration for the code and information in this notebook was taken from https://www.w3schools.com/python/python_intro.asp -- This is a great resource if you want to learn more about Python!-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Word Vectors\n",
    "\n",
    "**Text vectorization** is the process of converting texts into numbers or, more specifically, into vectors of numbers. \n",
    "\n",
    "[explain more]\n",
    "\n",
    "[what can we learn?]\n",
    "\n",
    "There are different methods of text vectorization. Three of the most common examples are:\n",
    "+ **Term Frequency - Inverse Document Frequency (TF-IDF)**:\n",
    "    + *Term frequency* is the number of times a word appears in one document. *Inverse document frequency** is - more or less - how frequently the word appears across the entire corpus in which this document is found. Thus, within a corpus of newspaper articles, an article on a baseball game will return high TF-IDF scores for words like \"hit\", \"run\", \"RBI\", and \"innings\" as well as the names of teams and individual players. But, common words found in that same article, like \"the\", \"this\", \"and\", etc. will have low TF-IDF scores. \n",
    "+ **Word2Vec**: \n",
    "    + Word2Vec is a method to convert a word to a numerical array that essentially situates the word into a multi-dimension language space where similar words are found close to one another. [more] [\"embeddings\"]\n",
    "    + applying this vectorization method to a corpus is significantly faster than the other two methods mentions here\n",
    "    + however, TF-IDF is a far simpler process to understand\n",
    "+ **Sentence-BERT**:\n",
    "    + Instead of creating a vector for each word, Sentence-BERT creates a vector for each sentence. This allows the encoding of a word's context: for example, that the *bow* of a ship is something altogether different from a *bow* that you tie or a *bow* and arrow.\n",
    "    + unlike Word2Vec and like TF-IDF, however, this method is computationally intensive and takes up a lot of memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II: Setup\n",
    "\n",
    "1. Before beginning, we need to import some packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "from pathlib import Path\n",
    "import glob \n",
    "import pandas as pd\n",
    "\n",
    "textdir = Path(\"~/shared/RR-workshop-data/state-of-the-union-dataset/txt\").expanduser() \n",
    "pathlist = sorted(textdir.glob('*.txt')) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Term Frequency - Inverse Data Frequency (TFIDF)\n",
    "\n",
    "<img src = \"https://miro.medium.com/max/720/1*qQgnyPLDIkUmeZKN2_ZWbQ.webp\" style=\"width:60%\">\n",
    "\n",
    "Image from Yassine Hamdaoui, [\"TF(Term Frequency)-IDF(Inverse Document Frequency) from scratch in python\"](https://towardsdatascience.com/tf-term-frequency-idf-inverse-document-frequency-from-scratch-in-python-6c2b61b78558) *Towards Data Science (Medium)* (Dec. 9, 2019).\n",
    "\n",
    "***Portions of this notebook are taken from the lesson [\"TF-IDF with Scikit-learn\"](https://melaniewalsh.github.io/Intro-Cultural-Analytics/05-Text-Analysis/03-TF-IDF-Scikit-Learn.html) in Melanie Walsh's Introduction to Cultural Analytics & Python book  (indicated by the [MW]).***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III. TF-IDF with Scikit-Learn [MW]\n",
    "\n",
    "Tf-idf is a method that tries to identify the most distinctively frequent or significant words in a document. \n",
    "\n",
    "In this lesson, we’re going to learn how to calculate tf-idf scores using a collection of plain text (.txt) files and the Python library scikit-learn, which has a quick and nifty module called TfidfVectorizer.\n",
    "\n",
    "In this lesson, we will cover how to:\n",
    "\n",
    "    Calculate and normalize tf-idf scores for U.S. Inaugural Addresses with scikit-learn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IV. Breaking Down the TF-IDF Formula [MW]\n",
    "\n",
    "But first, let’s quickly discuss the tf-idf formula. The idea is pretty simple.\n",
    "\n",
    "**tf-idf = term_frequency * inverse_document_frequency**\n",
    "\n",
    "**term_frequency** = number of times a given term appears in document\n",
    "\n",
    "**inverse_document_frequency** = log(total number of documents / number of documents with term) + 1*****\n",
    "\n",
    "You take the number of times a term occurs in a document (term frequency). Then you take the number of documents in which the same term occurs at least once divided by the total number of documents (document frequency), and you flip that fraction on its head (inverse document frequency). Then you multiply the two numbers together (term_frequency * inverse_document_frequency).\n",
    "\n",
    "The reason we take the inverse, or flipped fraction, of document frequency is to boost the rarer words that occur in relatively few documents. Think about the inverse document frequency for the word “said” vs the word “pigeon.” The term “said” appears in 13 (document frequency) of 24 (total documents) Lost in the City stories (24 / 13 –> a smaller inverse document frequency) while the term “pigeons” only occurs in 2 (document frequency) of the 24 stories (total documents) (24 / 2 –> a bigger inverse document frequency, a bigger tf-idf boost).\n",
    "\n",
    "*There are a bunch of slightly different ways that you can calculate inverse document frequency. The version of idf that we’re going to use is the scikit-learn default, which uses “smoothing” aka it adds a “1” to the numerator and denominator:\n",
    "\n",
    "**inverse_document_frequency** = log((1 + total_number_of_documents) / (number_of_documents_with_term +1)) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V. Calculate tf-idf [MW]\n",
    "\n",
    "To calculate tf–idf scores for every word, we’re going to use scikit-learn’s TfidfVectorizer.\n",
    "\n",
    "4. When you initialize TfidfVectorizer, you can choose to set it with different parameters. These parameters will change the way you calculate tf–idf.\n",
    "\n",
    "The recommended way to run TfidfVectorizer is with smoothing (smooth_idf = True) and normalization (norm='l2') turned on. These parameters will better account for differences in text length, and overall produce more meaningful tf–idf scores. Smoothing and L2 normalization are actually the default settings for TfidfVectorizer, so to turn them on, you don’t need to include any extra code at all.\n",
    "\n",
    "Initialize TfidfVectorizer with desired parameters (default smoothing and normalization).\n",
    "\n",
    "**Note: tfidf vectors can become very large even for a modest number of texts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(input='filename', stop_words='english')\n",
    "tfidf_vectorizer2 = TfidfVectorizer(input='filename', stop_words='english', max_df = 0.5, max_features=5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Run TfidfVectorizer on our text_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<233x25023 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 361183 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vector = tfidf_vectorizer.fit_transform(pathlist)\n",
    "tfidf_vector\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Make a DataFrame out of the resulting tf–idf vector, setting the “feature names” or words as columns and the titles as rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              00       000  0000  0001  001  002  003  004  005  006  ...  \\\n",
      "Adams_1797   0.0  0.000000   0.0   0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   \n",
      "Adams_1798   0.0  0.000000   0.0   0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   \n",
      "Adams_1799   0.0  0.000000   0.0   0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   \n",
      "Adams_1800   0.0  0.000000   0.0   0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   \n",
      "Adams_1825   0.0  0.271497   0.0   0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   \n",
      "...          ...       ...   ...   ...  ...  ...  ...  ...  ...  ...  ...   \n",
      "Wilson_1916  0.0  0.000000   0.0   0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   \n",
      "Wilson_1917  0.0  0.000000   0.0   0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   \n",
      "Wilson_1918  0.0  0.000000   0.0   0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   \n",
      "Wilson_1919  0.0  0.023909   0.0   0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   \n",
      "Wilson_1920  0.0  0.204717   0.0   0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   \n",
      "\n",
      "             zimbabwe  zimbabwean  zinc  zion  zollverein  zone  zones  \\\n",
      "Adams_1797        0.0         0.0   0.0   0.0         0.0   0.0    0.0   \n",
      "Adams_1798        0.0         0.0   0.0   0.0         0.0   0.0    0.0   \n",
      "Adams_1799        0.0         0.0   0.0   0.0         0.0   0.0    0.0   \n",
      "Adams_1800        0.0         0.0   0.0   0.0         0.0   0.0    0.0   \n",
      "Adams_1825        0.0         0.0   0.0   0.0         0.0   0.0    0.0   \n",
      "...               ...         ...   ...   ...         ...   ...    ...   \n",
      "Wilson_1916       0.0         0.0   0.0   0.0         0.0   0.0    0.0   \n",
      "Wilson_1917       0.0         0.0   0.0   0.0         0.0   0.0    0.0   \n",
      "Wilson_1918       0.0         0.0   0.0   0.0         0.0   0.0    0.0   \n",
      "Wilson_1919       0.0         0.0   0.0   0.0         0.0   0.0    0.0   \n",
      "Wilson_1920       0.0         0.0   0.0   0.0         0.0   0.0    0.0   \n",
      "\n",
      "             zoological  zooming  zuloaga  \n",
      "Adams_1797          0.0      0.0      0.0  \n",
      "Adams_1798          0.0      0.0      0.0  \n",
      "Adams_1799          0.0      0.0      0.0  \n",
      "Adams_1800          0.0      0.0      0.0  \n",
      "Adams_1825          0.0      0.0      0.0  \n",
      "...                 ...      ...      ...  \n",
      "Wilson_1916         0.0      0.0      0.0  \n",
      "Wilson_1917         0.0      0.0      0.0  \n",
      "Wilson_1918         0.0      0.0      0.0  \n",
      "Wilson_1919         0.0      0.0      0.0  \n",
      "Wilson_1920         0.0      0.0      0.0  \n",
      "\n",
      "[233 rows x 25023 columns]\n"
     ]
    }
   ],
   "source": [
    "text_titles = [path.stem for path in pathlist]\n",
    "#TfidfVectorizer returns a sparse matrix and that's why we have to call .toarray()  before proceeding.\n",
    "tfidf_df = pd.DataFrame(tfidf_vector.toarray(), index=text_titles, columns=tfidf_vectorizer.get_feature_names_out())\n",
    "#warning: get_feature_names will be depreciated; use get_feature_names_out instead\n",
    "   ##I made this fix in the code above\n",
    "print(tfidf_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>00</th>\n",
       "      <th>000</th>\n",
       "      <th>0000</th>\n",
       "      <th>0001</th>\n",
       "      <th>001</th>\n",
       "      <th>002</th>\n",
       "      <th>003</th>\n",
       "      <th>004</th>\n",
       "      <th>005</th>\n",
       "      <th>006</th>\n",
       "      <th>...</th>\n",
       "      <th>zimbabwe</th>\n",
       "      <th>zimbabwean</th>\n",
       "      <th>zinc</th>\n",
       "      <th>zion</th>\n",
       "      <th>zollverein</th>\n",
       "      <th>zone</th>\n",
       "      <th>zones</th>\n",
       "      <th>zoological</th>\n",
       "      <th>zooming</th>\n",
       "      <th>zuloaga</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Adams_1797</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Adams_1798</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Adams_1799</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Adams_1800</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Adams_1825</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.271497</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 25023 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             00       000  0000  0001  001  002  003  004  005  006  ...  \\\n",
       "Adams_1797  0.0  0.000000   0.0   0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   \n",
       "Adams_1798  0.0  0.000000   0.0   0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   \n",
       "Adams_1799  0.0  0.000000   0.0   0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   \n",
       "Adams_1800  0.0  0.000000   0.0   0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   \n",
       "Adams_1825  0.0  0.271497   0.0   0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   \n",
       "\n",
       "            zimbabwe  zimbabwean  zinc  zion  zollverein  zone  zones  \\\n",
       "Adams_1797       0.0         0.0   0.0   0.0         0.0   0.0    0.0   \n",
       "Adams_1798       0.0         0.0   0.0   0.0         0.0   0.0    0.0   \n",
       "Adams_1799       0.0         0.0   0.0   0.0         0.0   0.0    0.0   \n",
       "Adams_1800       0.0         0.0   0.0   0.0         0.0   0.0    0.0   \n",
       "Adams_1825       0.0         0.0   0.0   0.0         0.0   0.0    0.0   \n",
       "\n",
       "            zoological  zooming  zuloaga  \n",
       "Adams_1797         0.0      0.0      0.0  \n",
       "Adams_1798         0.0      0.0      0.0  \n",
       "Adams_1799         0.0      0.0      0.0  \n",
       "Adams_1800         0.0      0.0      0.0  \n",
       "Adams_1825         0.0      0.0      0.0  \n",
       "\n",
       "[5 rows x 25023 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textname</th>\n",
       "      <th>00</th>\n",
       "      <th>000</th>\n",
       "      <th>0000</th>\n",
       "      <th>0001</th>\n",
       "      <th>001</th>\n",
       "      <th>002</th>\n",
       "      <th>003</th>\n",
       "      <th>004</th>\n",
       "      <th>005</th>\n",
       "      <th>...</th>\n",
       "      <th>zimbabwe</th>\n",
       "      <th>zimbabwean</th>\n",
       "      <th>zinc</th>\n",
       "      <th>zion</th>\n",
       "      <th>zollverein</th>\n",
       "      <th>zone</th>\n",
       "      <th>zones</th>\n",
       "      <th>zoological</th>\n",
       "      <th>zooming</th>\n",
       "      <th>zuloaga</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Adams_1797</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Adams_1798</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Adams_1799</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Adams_1800</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Adams_1825</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.271497</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 25024 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     textname   00       000  0000  0001  001  002  003  004  005  ...  \\\n",
       "0  Adams_1797  0.0  0.000000   0.0   0.0  0.0  0.0  0.0  0.0  0.0  ...   \n",
       "1  Adams_1798  0.0  0.000000   0.0   0.0  0.0  0.0  0.0  0.0  0.0  ...   \n",
       "2  Adams_1799  0.0  0.000000   0.0   0.0  0.0  0.0  0.0  0.0  0.0  ...   \n",
       "3  Adams_1800  0.0  0.000000   0.0   0.0  0.0  0.0  0.0  0.0  0.0  ...   \n",
       "4  Adams_1825  0.0  0.271497   0.0   0.0  0.0  0.0  0.0  0.0  0.0  ...   \n",
       "\n",
       "   zimbabwe  zimbabwean  zinc  zion  zollverein  zone  zones  zoological  \\\n",
       "0       0.0         0.0   0.0   0.0         0.0   0.0    0.0         0.0   \n",
       "1       0.0         0.0   0.0   0.0         0.0   0.0    0.0         0.0   \n",
       "2       0.0         0.0   0.0   0.0         0.0   0.0    0.0         0.0   \n",
       "3       0.0         0.0   0.0   0.0         0.0   0.0    0.0         0.0   \n",
       "4       0.0         0.0   0.0   0.0         0.0   0.0    0.0         0.0   \n",
       "\n",
       "   zooming  zuloaga  \n",
       "0      0.0      0.0  \n",
       "1      0.0      0.0  \n",
       "2      0.0      0.0  \n",
       "3      0.0      0.0  \n",
       "4      0.0      0.0  \n",
       "\n",
       "[5 rows x 25024 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_df.index.name = \"textname\"\n",
    "tfidf_df = tfidf_df.reset_index()\n",
    "tfidf_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textname</th>\n",
       "      <th>word</th>\n",
       "      <th>tfidf_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Adams_1797</td>\n",
       "      <td>00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Adams_1798</td>\n",
       "      <td>00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Adams_1799</td>\n",
       "      <td>00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Adams_1800</td>\n",
       "      <td>00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Adams_1825</td>\n",
       "      <td>00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     textname word  tfidf_score\n",
       "0  Adams_1797   00          0.0\n",
       "1  Adams_1798   00          0.0\n",
       "2  Adams_1799   00          0.0\n",
       "3  Adams_1800   00          0.0\n",
       "4  Adams_1825   00          0.0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_long =  pd.melt(tfidf_df, id_vars = \"textname\", var_name = \"word\", value_name = \"tfidf_score\", value_vars = list(tfidf_df.drop(columns = [\"textname\"]).columns))\n",
    "tfidf_long.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5830359, 3)\n",
      "(361183, 3)\n"
     ]
    }
   ],
   "source": [
    "print(tfidf_long.shape)\n",
    "tfidf_long = tfidf_long[tfidf_long['tfidf_score'] > 0.0]\n",
    "print(tfidf_long.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(361183, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textname</th>\n",
       "      <th>word</th>\n",
       "      <th>tfidf_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Taft_1910</td>\n",
       "      <td>00</td>\n",
       "      <td>0.648663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Johnson_1966</td>\n",
       "      <td>vietnam</td>\n",
       "      <td>0.523858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Carter_1980</td>\n",
       "      <td>soviet</td>\n",
       "      <td>0.475029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Cleveland_1895</td>\n",
       "      <td>gold</td>\n",
       "      <td>0.464477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Arthur_1883</td>\n",
       "      <td>00</td>\n",
       "      <td>0.458837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Arthur_1882</td>\n",
       "      <td>00</td>\n",
       "      <td>0.452926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Polk_1846</td>\n",
       "      <td>mexico</td>\n",
       "      <td>0.442578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Eisenhower_1961</td>\n",
       "      <td>1953</td>\n",
       "      <td>0.414143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Hoover_1930</td>\n",
       "      <td>000</td>\n",
       "      <td>0.405701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Pierce_1855</td>\n",
       "      <td>states</td>\n",
       "      <td>0.399066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Hoover_1929</td>\n",
       "      <td>000</td>\n",
       "      <td>0.394602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Harrison_1892</td>\n",
       "      <td>1892</td>\n",
       "      <td>0.387563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Polk_1847</td>\n",
       "      <td>mexico</td>\n",
       "      <td>0.387447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Adams_1826</td>\n",
       "      <td>000</td>\n",
       "      <td>0.378096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Johnson_1865</td>\n",
       "      <td>states</td>\n",
       "      <td>0.375645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Pierce_1856</td>\n",
       "      <td>states</td>\n",
       "      <td>0.359985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Polk_1846</td>\n",
       "      <td>texas</td>\n",
       "      <td>0.353746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Monroe_1819</td>\n",
       "      <td>spain</td>\n",
       "      <td>0.350451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Adams_1800</td>\n",
       "      <td>gentlemen</td>\n",
       "      <td>0.345601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Roosevelt_1944</td>\n",
       "      <td>war</td>\n",
       "      <td>0.344661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Truman_1946</td>\n",
       "      <td>1947</td>\n",
       "      <td>0.340787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Clinton_1996</td>\n",
       "      <td>challenge</td>\n",
       "      <td>0.333385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Johnson_1866</td>\n",
       "      <td>states</td>\n",
       "      <td>0.331233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Bush_2003</td>\n",
       "      <td>hussein</td>\n",
       "      <td>0.326777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Bush_2003</td>\n",
       "      <td>saddam</td>\n",
       "      <td>0.326777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Johnson_1967</td>\n",
       "      <td>vietnam</td>\n",
       "      <td>0.317941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Truman_1953</td>\n",
       "      <td>world</td>\n",
       "      <td>0.316771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>McKinley_1897</td>\n",
       "      <td>government</td>\n",
       "      <td>0.315000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Coolidge_1928</td>\n",
       "      <td>000</td>\n",
       "      <td>0.313434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Monroe_1818</td>\n",
       "      <td>spain</td>\n",
       "      <td>0.310647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Roosevelt_1937</td>\n",
       "      <td>democracy</td>\n",
       "      <td>0.310207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Roosevelt_1943</td>\n",
       "      <td>1942</td>\n",
       "      <td>0.307951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Arthur_1881</td>\n",
       "      <td>00</td>\n",
       "      <td>0.307080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Truman_1946</td>\n",
       "      <td>dollars</td>\n",
       "      <td>0.305981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>Coolidge_1927</td>\n",
       "      <td>000</td>\n",
       "      <td>0.305078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Adams_1827</td>\n",
       "      <td>1827</td>\n",
       "      <td>0.303332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>Johnson_1968</td>\n",
       "      <td>billion</td>\n",
       "      <td>0.301968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>Washington_1793</td>\n",
       "      <td>states</td>\n",
       "      <td>0.301163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>Roosevelt_1942</td>\n",
       "      <td>hitler</td>\n",
       "      <td>0.300475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>Hoover_1930</td>\n",
       "      <td>1928</td>\n",
       "      <td>0.300244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>Truman_1951</td>\n",
       "      <td>soviet</td>\n",
       "      <td>0.299944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>Grant_1870</td>\n",
       "      <td>states</td>\n",
       "      <td>0.299520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>Madison_1814</td>\n",
       "      <td>enemy</td>\n",
       "      <td>0.296641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>Johnson_1868</td>\n",
       "      <td>000</td>\n",
       "      <td>0.295729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>Biden_2021</td>\n",
       "      <td>jobs</td>\n",
       "      <td>0.295617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>Bush_2008</td>\n",
       "      <td>iraq</td>\n",
       "      <td>0.291001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>Eisenhower_1956</td>\n",
       "      <td>program</td>\n",
       "      <td>0.290541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>Buren_1839</td>\n",
       "      <td>banks</td>\n",
       "      <td>0.290191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>Cleveland_1887</td>\n",
       "      <td>tariff</td>\n",
       "      <td>0.288231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>Hoover_1931</td>\n",
       "      <td>banks</td>\n",
       "      <td>0.287929</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           textname        word  tfidf_score\n",
       "0         Taft_1910          00     0.648663\n",
       "1      Johnson_1966     vietnam     0.523858\n",
       "2       Carter_1980      soviet     0.475029\n",
       "3    Cleveland_1895        gold     0.464477\n",
       "4       Arthur_1883          00     0.458837\n",
       "5       Arthur_1882          00     0.452926\n",
       "6         Polk_1846      mexico     0.442578\n",
       "7   Eisenhower_1961        1953     0.414143\n",
       "8       Hoover_1930         000     0.405701\n",
       "9       Pierce_1855      states     0.399066\n",
       "10      Hoover_1929         000     0.394602\n",
       "11    Harrison_1892        1892     0.387563\n",
       "12        Polk_1847      mexico     0.387447\n",
       "13       Adams_1826         000     0.378096\n",
       "14     Johnson_1865      states     0.375645\n",
       "15      Pierce_1856      states     0.359985\n",
       "16        Polk_1846       texas     0.353746\n",
       "17      Monroe_1819       spain     0.350451\n",
       "18       Adams_1800   gentlemen     0.345601\n",
       "19   Roosevelt_1944         war     0.344661\n",
       "20      Truman_1946        1947     0.340787\n",
       "21     Clinton_1996   challenge     0.333385\n",
       "22     Johnson_1866      states     0.331233\n",
       "23        Bush_2003     hussein     0.326777\n",
       "24        Bush_2003      saddam     0.326777\n",
       "25     Johnson_1967     vietnam     0.317941\n",
       "26      Truman_1953       world     0.316771\n",
       "27    McKinley_1897  government     0.315000\n",
       "28    Coolidge_1928         000     0.313434\n",
       "29      Monroe_1818       spain     0.310647\n",
       "30   Roosevelt_1937   democracy     0.310207\n",
       "31   Roosevelt_1943        1942     0.307951\n",
       "32      Arthur_1881          00     0.307080\n",
       "33      Truman_1946     dollars     0.305981\n",
       "34    Coolidge_1927         000     0.305078\n",
       "35       Adams_1827        1827     0.303332\n",
       "36     Johnson_1968     billion     0.301968\n",
       "37  Washington_1793      states     0.301163\n",
       "38   Roosevelt_1942      hitler     0.300475\n",
       "39      Hoover_1930        1928     0.300244\n",
       "40      Truman_1951      soviet     0.299944\n",
       "41       Grant_1870      states     0.299520\n",
       "42     Madison_1814       enemy     0.296641\n",
       "43     Johnson_1868         000     0.295729\n",
       "44       Biden_2021        jobs     0.295617\n",
       "45        Bush_2008        iraq     0.291001\n",
       "46  Eisenhower_1956     program     0.290541\n",
       "47       Buren_1839       banks     0.290191\n",
       "48   Cleveland_1887      tariff     0.288231\n",
       "49      Hoover_1931       banks     0.287929"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get top 15 tfidf scores for each text\n",
    "N = 15\n",
    "tfidf_long = tfidf_long.sort_values(by = \"tfidf_score\", ascending=False)\n",
    "print(tfidf_long.shape)\n",
    "tfidf_sub = tfidf_long.groupby('textname').head(N).reset_index(drop=True)\n",
    "\n",
    "tfidf_sub.head(50)\n",
    "\n",
    "#textnames = list(tfidf_long['textname'].unique())\n",
    "\n",
    "#for i, text in enumerate(textnames):\n",
    "#    onetext_df = tfidf_sub[tfidf_sub['textname'] == text]\n",
    "#    print(onetext_df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" role=\"alert\" style=\"color:blue\">\n",
    "    <h3><b>Exercises</b>:</h3> \n",
    "    <p>7. Subset the tfidf_sub dataframe to examine the top tfidf_scores for one particular speech</p>\n",
    "    <p>7b Advanced. Subset the tfidf_sub dataframe to examine the top tfidf_scores for a president.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automate the TF-IDF vectorization process with a function\n",
    "\n",
    "8. The function below integrates some of the tasks we did above to further automate the process of creating tf-idf vectors. \n",
    "\n",
    "Examine the `tfidf_analysis` function below. What are its inputs? What does it return (its output)? Can you identify what each line of code does?\n",
    "\n",
    "You may notice, that this code applies an additional processing step to our text: it lemmatizes tokens from the text, reducing words to their base form (plural --> singular for nouns, present tense first-person for verbs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer   ###\n",
    "from nltk.corpus import stopwords\n",
    "stop = sorted(stopwords.words('english'))\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer(r'\\w+')   \n",
    "\n",
    "# Interface lemma tokenizer from nltk with sklearn\n",
    "class LemmaTokenizer:                                               ###\n",
    "    #ignore_tokens = [',', '.', ';', ':', '\"', '``', \"''\", '`']      ###\n",
    "    def __init__(self):                                             ###\n",
    "        self.wnl = WordNetLemmatizer()                              ###\n",
    "    def __call__(self, doc):                                        ###\n",
    "        #return [self.wnl.lemmatize(t) for t in word_tokenize(doc) if t not in self.ignore_tokens]\n",
    "        return [self.wnl.lemmatize(t) for t in tokenizer.tokenize(doc) if not t.isdigit()] # if t not in self.ignore_tokens]    ###\n",
    "    \n",
    "lemma_tokenizer = LemmaTokenizer()                                 ###\n",
    "eng_stops = set(stopwords.words('english'))                        ###\n",
    "lemma_stop = lemma_tokenizer(' '.join(eng_stops))                  ###\n",
    "\n",
    "def tfidf_analysis(textdir, ng_range = (1,1), lemmas = False):\n",
    "    '''\n",
    "    textdir = pathlib Path object to folder containing .txt files to be analyzed\n",
    "    ng_range = range of ngrams to be analyzed, i.e. (1,2) will analyze words of length 1 (unigrams) and 2 (bigrams) \n",
    "    reads in a file folder and returns a long tfidf dataframe for all .txt files found in this folder\n",
    "    Steps:\n",
    "    1. \n",
    "    '''\n",
    "    #tfidf_vectorizer = TfidfVectorizer(input='filename', stop_words='english', ngram_range = (ng_range))\n",
    "    if lemmas:\n",
    "        tfidf_vectorizer = TfidfVectorizer(input = \"filename\", stop_words = lemma_stop, tokenizer = lemma_tokenizer, ngram_range = (ng_range), max_df = 0.5, max_features=5000)  #$$$$\n",
    "    else:\n",
    "        tfidf_vectorizer = TfidfVectorizer(input = \"filename\", stop_words = \"english\", ngram_range = (ng_range), max_df = 0.5, max_features=5000)  #$$$$\n",
    "        \n",
    "    pathlist = sorted(textdir.glob('*.txt'))\n",
    "    tfidf_vector = tfidf_vectorizer.fit_transform(pathlist)\n",
    "    text_titles = [path.stem for path in pathlist]\n",
    "    tfidf_df = pd.DataFrame(tfidf_vector.toarray(), index=text_titles, columns=tfidf_vectorizer.get_feature_names_out())\n",
    "    print(\"df shape: \", tfidf_df.shape)\n",
    "    tfidf_df = tfidf_df.loc[: ,(tfidf_df.max(numeric_only = True) > 0.02)]\n",
    "    print(\"df shape: \", tfidf_df.shape)\n",
    "    #print(tfidf_df.head())\n",
    "    tfidf_df.index.name = \"textname\"\n",
    "    tfidf_df = tfidf_df.reset_index()\n",
    "    tfidf_long =  pd.melt(tfidf_df, id_vars = \"textname\", var_name = \"word\", value_name = \"tfidf_score\", value_vars = list(tfidf_df.drop(columns = [\"textname\"]).columns))\n",
    "    \n",
    "    tfidf_long = tfidf_long.sort_values(by = 'tfidf_score', ascending = False)\n",
    "    print(\"df shape: \", tfidf_long.shape)\n",
    "    #print(tfidf_long.head(10))\n",
    "    return(tfidf_long)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. We can call the function below. You can try it with or without lemmatization and with single words or n-grams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\F0040RP\\Documents\\DartLib_RDS\\intro-to-python\\.venv\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df shape:  (233, 5000)\n",
      "df shape:  (233, 5000)\n",
      "df shape:  (1165000, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textname</th>\n",
       "      <th>word</th>\n",
       "      <th>tfidf_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1125509</th>\n",
       "      <td>Johnson_1966</td>\n",
       "      <td>vietnam</td>\n",
       "      <td>0.667941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>987957</th>\n",
       "      <td>Carter_1980</td>\n",
       "      <td>soviet</td>\n",
       "      <td>0.633374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500529</th>\n",
       "      <td>Cleveland_1895</td>\n",
       "      <td>gold</td>\n",
       "      <td>0.593359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>683324</th>\n",
       "      <td>Polk_1847</td>\n",
       "      <td>mexico</td>\n",
       "      <td>0.566879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>683323</th>\n",
       "      <td>Polk_1846</td>\n",
       "      <td>mexico</td>\n",
       "      <td>0.552646</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               textname     word  tfidf_score\n",
       "1125509    Johnson_1966  vietnam     0.667941\n",
       "987957      Carter_1980   soviet     0.633374\n",
       "500529   Cleveland_1895     gold     0.593359\n",
       "683324        Polk_1847   mexico     0.566879\n",
       "683323        Polk_1846   mexico     0.552646"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#take 4x longer to run with lemmatizing tokenizer!\n",
    "longdf = tfidf_analysis(textdir, ng_range = (1,1), lemmas = True)\n",
    "longdf.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. The above dataframe is large: it has 1.1 million rows. We can reduce it by just keeping the top *N* words by tfidf score for each president. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2330, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textname</th>\n",
       "      <th>word</th>\n",
       "      <th>tfidf_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Johnson_1966</td>\n",
       "      <td>vietnam</td>\n",
       "      <td>0.667941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Carter_1980</td>\n",
       "      <td>soviet</td>\n",
       "      <td>0.633374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Cleveland_1895</td>\n",
       "      <td>gold</td>\n",
       "      <td>0.593359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Polk_1847</td>\n",
       "      <td>mexico</td>\n",
       "      <td>0.566879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Polk_1846</td>\n",
       "      <td>mexico</td>\n",
       "      <td>0.552646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Clinton_1996</td>\n",
       "      <td>challenge</td>\n",
       "      <td>0.551497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Truman_1953</td>\n",
       "      <td>communist</td>\n",
       "      <td>0.525939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Reagan_1982</td>\n",
       "      <td>program</td>\n",
       "      <td>0.520346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Obama_2013</td>\n",
       "      <td>job</td>\n",
       "      <td>0.495715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Roosevelt_1937</td>\n",
       "      <td>democracy</td>\n",
       "      <td>0.489216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Eisenhower_1956</td>\n",
       "      <td>program</td>\n",
       "      <td>0.485257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Monroe_1819</td>\n",
       "      <td>spain</td>\n",
       "      <td>0.468038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Adams_1800</td>\n",
       "      <td>gentleman</td>\n",
       "      <td>0.466061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Eisenhower_1955</td>\n",
       "      <td>program</td>\n",
       "      <td>0.446461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Johnson_1967</td>\n",
       "      <td>vietnam</td>\n",
       "      <td>0.443372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Polk_1846</td>\n",
       "      <td>texas</td>\n",
       "      <td>0.441722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Truman_1951</td>\n",
       "      <td>soviet</td>\n",
       "      <td>0.441560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Obama_2012</td>\n",
       "      <td>job</td>\n",
       "      <td>0.440209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Biden_2021</td>\n",
       "      <td>job</td>\n",
       "      <td>0.436227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Bush_2002</td>\n",
       "      <td>terrorist</td>\n",
       "      <td>0.425527</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           textname       word  tfidf_score\n",
       "0      Johnson_1966    vietnam     0.667941\n",
       "1       Carter_1980     soviet     0.633374\n",
       "2    Cleveland_1895       gold     0.593359\n",
       "3         Polk_1847     mexico     0.566879\n",
       "4         Polk_1846     mexico     0.552646\n",
       "5      Clinton_1996  challenge     0.551497\n",
       "6       Truman_1953  communist     0.525939\n",
       "7       Reagan_1982    program     0.520346\n",
       "8        Obama_2013        job     0.495715\n",
       "9    Roosevelt_1937  democracy     0.489216\n",
       "10  Eisenhower_1956    program     0.485257\n",
       "11      Monroe_1819      spain     0.468038\n",
       "12       Adams_1800  gentleman     0.466061\n",
       "13  Eisenhower_1955    program     0.446461\n",
       "14     Johnson_1967    vietnam     0.443372\n",
       "15        Polk_1846      texas     0.441722\n",
       "16      Truman_1951     soviet     0.441560\n",
       "17       Obama_2012        job     0.440209\n",
       "18       Biden_2021        job     0.436227\n",
       "19        Bush_2002  terrorist     0.425527"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N = 10\n",
    "longdf_sub = longdf.groupby('textname').head(N).reset_index(drop=True)\n",
    "print(longdf_sub.shape)\n",
    "longdf_sub.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" role=\"alert\" style=\"color:blue\">\n",
    "    <h3><b>Exercises</b>:</h3> \n",
    "    <p>11. To view the dataframe differently we can sort it by first year and then tfidf_score. Do so below:</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textname</th>\n",
       "      <th>word</th>\n",
       "      <th>tfidf_score</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>Biden_2023</td>\n",
       "      <td>folk</td>\n",
       "      <td>0.326948</td>\n",
       "      <td>2023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>Biden_2023</td>\n",
       "      <td>going</td>\n",
       "      <td>0.315507</td>\n",
       "      <td>2023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>Biden_2023</td>\n",
       "      <td>job</td>\n",
       "      <td>0.269779</td>\n",
       "      <td>2023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>558</th>\n",
       "      <td>Biden_2023</td>\n",
       "      <td>get</td>\n",
       "      <td>0.176336</td>\n",
       "      <td>2023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>734</th>\n",
       "      <td>Biden_2023</td>\n",
       "      <td>cancer</td>\n",
       "      <td>0.161842</td>\n",
       "      <td>2023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>761</th>\n",
       "      <td>Biden_2023</td>\n",
       "      <td>drug</td>\n",
       "      <td>0.158991</td>\n",
       "      <td>2023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>977</th>\n",
       "      <td>Biden_2023</td>\n",
       "      <td>tonight</td>\n",
       "      <td>0.145714</td>\n",
       "      <td>2023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1084</th>\n",
       "      <td>Biden_2023</td>\n",
       "      <td>finish</td>\n",
       "      <td>0.140049</td>\n",
       "      <td>2023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1250</th>\n",
       "      <td>Biden_2023</td>\n",
       "      <td>medicare</td>\n",
       "      <td>0.132981</td>\n",
       "      <td>2023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1463</th>\n",
       "      <td>Biden_2023</td>\n",
       "      <td>covid</td>\n",
       "      <td>0.125162</td>\n",
       "      <td>2023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>Biden_2022</td>\n",
       "      <td>folk</td>\n",
       "      <td>0.274183</td>\n",
       "      <td>2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>Biden_2022</td>\n",
       "      <td>covid</td>\n",
       "      <td>0.246229</td>\n",
       "      <td>2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240</th>\n",
       "      <td>Biden_2022</td>\n",
       "      <td>putin</td>\n",
       "      <td>0.235389</td>\n",
       "      <td>2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>280</th>\n",
       "      <td>Biden_2022</td>\n",
       "      <td>tonight</td>\n",
       "      <td>0.226236</td>\n",
       "      <td>2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>404</th>\n",
       "      <td>Biden_2022</td>\n",
       "      <td>get</td>\n",
       "      <td>0.194576</td>\n",
       "      <td>2022</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        textname      word  tfidf_score  year\n",
       "71    Biden_2023      folk     0.326948  2023\n",
       "82    Biden_2023     going     0.315507  2023\n",
       "144   Biden_2023       job     0.269779  2023\n",
       "558   Biden_2023       get     0.176336  2023\n",
       "734   Biden_2023    cancer     0.161842  2023\n",
       "761   Biden_2023      drug     0.158991  2023\n",
       "977   Biden_2023   tonight     0.145714  2023\n",
       "1084  Biden_2023    finish     0.140049  2023\n",
       "1250  Biden_2023  medicare     0.132981  2023\n",
       "1463  Biden_2023     covid     0.125162  2023\n",
       "129   Biden_2022      folk     0.274183  2022\n",
       "195   Biden_2022     covid     0.246229  2022\n",
       "240   Biden_2022     putin     0.235389  2022\n",
       "280   Biden_2022   tonight     0.226236  2022\n",
       "404   Biden_2022       get     0.194576  2022"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "longdf_sub['year'] = longdf_sub['textname'].str[-4:].astype(int)\n",
    "longdf_sub = longdf_sub.sort_values(by = [\"year\", \"tfidf_score\"], ascending = [False, False])\n",
    "longdf_sub.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" role=\"alert\" style=\"color:blue\">\n",
    "    <h3><b>Exercises</b></h3>\n",
    "    <p>12. Subset the dataframe by year, keeping only those speeches given on or after the year 2000.<p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "longdf_21C = longdf_sub.loc[(longdf_sub['year'] >= 2000), :]\n",
    "longdf_21C = longdf_21C.sort_values(by = \"year\", ascending = False)\n",
    "longdf_21C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VI. TFIDF vectors with ngrams\n",
    "\n",
    "13. With the function `tfidf_analysis` we can compile tfidf_scores for ngrams, including two-, three-, and four-word terms by adjusting the minimum and maximum ngram length in the tuple called by the parameter `ng_range`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\F0040RP\\Documents\\DartLib_RDS\\intro-to-python\\.venv\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df shape:  (233, 5000)\n",
      "df shape:  (233, 5000)\n",
      "df shape:  (1165000, 3)\n",
      "(3495, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textname</th>\n",
       "      <th>word</th>\n",
       "      <th>tfidf_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bush_2003</td>\n",
       "      <td>saddam hussein</td>\n",
       "      <td>0.683307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Madison_1815</td>\n",
       "      <td>sum million</td>\n",
       "      <td>0.589258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Clinton_1994</td>\n",
       "      <td>health care</td>\n",
       "      <td>0.570977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Truman_1953</td>\n",
       "      <td>free world</td>\n",
       "      <td>0.570115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Madison_1813</td>\n",
       "      <td>prisoner war</td>\n",
       "      <td>0.566379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Truman_1951</td>\n",
       "      <td>free nation</td>\n",
       "      <td>0.540958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Eisenhower_1960</td>\n",
       "      <td>free world</td>\n",
       "      <td>0.535015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Bush_2008</td>\n",
       "      <td>must trust</td>\n",
       "      <td>0.531823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Johnson_1966</td>\n",
       "      <td>south vietnam</td>\n",
       "      <td>0.530202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Biden_2021</td>\n",
       "      <td>job plan</td>\n",
       "      <td>0.528341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Carter_1980</td>\n",
       "      <td>soviet union</td>\n",
       "      <td>0.524191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Roosevelt_1942</td>\n",
       "      <td>year shall</td>\n",
       "      <td>0.521986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Clinton_1999</td>\n",
       "      <td>21st century</td>\n",
       "      <td>0.505364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Clinton_1995</td>\n",
       "      <td>new covenant</td>\n",
       "      <td>0.503915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Roosevelt_1944</td>\n",
       "      <td>national service</td>\n",
       "      <td>0.501944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Roosevelt_1938</td>\n",
       "      <td>purchasing power</td>\n",
       "      <td>0.500852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Clinton_1996</td>\n",
       "      <td>challenge congress</td>\n",
       "      <td>0.500062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Truman_1947</td>\n",
       "      <td>labor management</td>\n",
       "      <td>0.479798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Truman_1946</td>\n",
       "      <td>million dollar</td>\n",
       "      <td>0.466839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Grant_1870</td>\n",
       "      <td>san domingo</td>\n",
       "      <td>0.462534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Bush_2007</td>\n",
       "      <td>al qaeda</td>\n",
       "      <td>0.456074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Bush_2005</td>\n",
       "      <td>social security</td>\n",
       "      <td>0.453371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Trump_2020</td>\n",
       "      <td>thank much</td>\n",
       "      <td>0.452914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Nixon_1971</td>\n",
       "      <td>great goal</td>\n",
       "      <td>0.440587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Roosevelt_1943</td>\n",
       "      <td>time greater</td>\n",
       "      <td>0.440108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Obama_2009</td>\n",
       "      <td>health care</td>\n",
       "      <td>0.433938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Hoover_1930</td>\n",
       "      <td>construction work</td>\n",
       "      <td>0.428987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Harrison_1892</td>\n",
       "      <td>per cent</td>\n",
       "      <td>0.427926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Eisenhower_1961</td>\n",
       "      <td>eight year</td>\n",
       "      <td>0.409540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Bush_2008</td>\n",
       "      <td>al qaeda</td>\n",
       "      <td>0.402828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Bush_2001</td>\n",
       "      <td>social security</td>\n",
       "      <td>0.401968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Clinton_1993</td>\n",
       "      <td>health care</td>\n",
       "      <td>0.397927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Roosevelt_1939</td>\n",
       "      <td>billion dollar</td>\n",
       "      <td>0.393257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Johnson_1865</td>\n",
       "      <td>general government</td>\n",
       "      <td>0.392808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>Roosevelt_1943</td>\n",
       "      <td>united nation</td>\n",
       "      <td>0.389242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Wilson_1917</td>\n",
       "      <td>austria hungary</td>\n",
       "      <td>0.388297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>Biden_2022</td>\n",
       "      <td>audience member</td>\n",
       "      <td>0.387299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>Trump_2019</td>\n",
       "      <td>thank much</td>\n",
       "      <td>0.386730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>Clinton_1998</td>\n",
       "      <td>21st century</td>\n",
       "      <td>0.384368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>Monroe_1819</td>\n",
       "      <td>catholic majesty</td>\n",
       "      <td>0.383523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>Cleveland_1895</td>\n",
       "      <td>united state note</td>\n",
       "      <td>0.382398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>Cleveland_1895</td>\n",
       "      <td>state note</td>\n",
       "      <td>0.382398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>Clinton_1999</td>\n",
       "      <td>social security</td>\n",
       "      <td>0.382039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>Nixon_1971</td>\n",
       "      <td>let u</td>\n",
       "      <td>0.378535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>Roosevelt_1904</td>\n",
       "      <td>forest reserve</td>\n",
       "      <td>0.373737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>Buren_1839</td>\n",
       "      <td>public money</td>\n",
       "      <td>0.372109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>Biden_2023</td>\n",
       "      <td>finish job</td>\n",
       "      <td>0.369823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>Eisenhower_1957</td>\n",
       "      <td>free world</td>\n",
       "      <td>0.368017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>Roosevelt_1942</td>\n",
       "      <td>united nation</td>\n",
       "      <td>0.367030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>Trump_2019</td>\n",
       "      <td>audience member</td>\n",
       "      <td>0.366198</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           textname                word  tfidf_score\n",
       "0         Bush_2003      saddam hussein     0.683307\n",
       "1      Madison_1815         sum million     0.589258\n",
       "2      Clinton_1994         health care     0.570977\n",
       "3       Truman_1953          free world     0.570115\n",
       "4      Madison_1813        prisoner war     0.566379\n",
       "5       Truman_1951         free nation     0.540958\n",
       "6   Eisenhower_1960          free world     0.535015\n",
       "7         Bush_2008          must trust     0.531823\n",
       "8      Johnson_1966       south vietnam     0.530202\n",
       "9        Biden_2021            job plan     0.528341\n",
       "10      Carter_1980        soviet union     0.524191\n",
       "11   Roosevelt_1942          year shall     0.521986\n",
       "12     Clinton_1999        21st century     0.505364\n",
       "13     Clinton_1995        new covenant     0.503915\n",
       "14   Roosevelt_1944    national service     0.501944\n",
       "15   Roosevelt_1938    purchasing power     0.500852\n",
       "16     Clinton_1996  challenge congress     0.500062\n",
       "17      Truman_1947    labor management     0.479798\n",
       "18      Truman_1946      million dollar     0.466839\n",
       "19       Grant_1870         san domingo     0.462534\n",
       "20        Bush_2007            al qaeda     0.456074\n",
       "21        Bush_2005     social security     0.453371\n",
       "22       Trump_2020          thank much     0.452914\n",
       "23       Nixon_1971          great goal     0.440587\n",
       "24   Roosevelt_1943        time greater     0.440108\n",
       "25       Obama_2009         health care     0.433938\n",
       "26      Hoover_1930   construction work     0.428987\n",
       "27    Harrison_1892            per cent     0.427926\n",
       "28  Eisenhower_1961          eight year     0.409540\n",
       "29        Bush_2008            al qaeda     0.402828\n",
       "30        Bush_2001     social security     0.401968\n",
       "31     Clinton_1993         health care     0.397927\n",
       "32   Roosevelt_1939      billion dollar     0.393257\n",
       "33     Johnson_1865  general government     0.392808\n",
       "34   Roosevelt_1943       united nation     0.389242\n",
       "35      Wilson_1917     austria hungary     0.388297\n",
       "36       Biden_2022     audience member     0.387299\n",
       "37       Trump_2019          thank much     0.386730\n",
       "38     Clinton_1998        21st century     0.384368\n",
       "39      Monroe_1819    catholic majesty     0.383523\n",
       "40   Cleveland_1895   united state note     0.382398\n",
       "41   Cleveland_1895          state note     0.382398\n",
       "42     Clinton_1999     social security     0.382039\n",
       "43       Nixon_1971               let u     0.378535\n",
       "44   Roosevelt_1904      forest reserve     0.373737\n",
       "45       Buren_1839        public money     0.372109\n",
       "46       Biden_2023          finish job     0.369823\n",
       "47  Eisenhower_1957          free world     0.368017\n",
       "48   Roosevelt_1942       united nation     0.367030\n",
       "49       Trump_2019     audience member     0.366198"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N = 15\n",
    "min_ng = 2\n",
    "max_ng = 3\n",
    "ng_longdf = tfidf_analysis(textdir, ng_range = (min_ng, max_ng), lemmas = True)\n",
    "ng_longdf = ng_longdf.sort_values(by = \"tfidf_score\", ascending=False)\n",
    "ng_longdf_sub = ng_longdf.groupby('textname').head(N).reset_index(drop=True)\n",
    "print(ng_longdf_sub.shape)\n",
    "ng_longdf_sub.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "ng_longdf_sub.to_csv(f\"sotu_{min_ng}-{max_ng}grams_tfidf_top{N}.csv\", encoding = 'utf-8')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
