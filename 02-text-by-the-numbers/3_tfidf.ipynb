{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text by the Numbers: Word Vectors\n",
    "\n",
    "**A Reproducible Research Workshop**\n",
    "\n",
    "(A Collaboration between Dartmouth Library and Research Computing)\n",
    "\n",
    "[*Click here to view or register for our current list of workshops*](http://dartgo.org/RRADworkshops)\n",
    "\n",
    "*This notebook created by*:\n",
    "+ Version 1.0: Jeremy Mikecz, Research Data Services (Dartmouth Library)\n",
    "<!--\n",
    "+ Some of the inspiration for the code and information in this notebook was taken from https://www.w3schools.com/python/python_intro.asp -- This is a great resource if you want to learn more about Python!-->\n",
    "\n",
    "This is **Notebook 3** of 3 for the **Text by the Numbers: Text Analysis Methods in Python** workshop:\n",
    "+ Notebook 1: Text by the Numbers: Working with one text\n",
    "+ Notebook 2: Text by the Numbers 2: Working with a Corpus\n",
    "+ **Notebook 3: Text by the Numbers 3: Word Vectors**\n",
    "\n",
    "In this session, participants will:\n",
    "\n",
    "+ Learn about text vectorizations: common methods and their uses\n",
    "+ run and analyze the code that creates TF-IDF vectors from a corpus\n",
    "+ analyze the results produced by this analysis \n",
    "\n",
    "**Table of Contents**\n",
    "+ I. Word Vectors (an Introduction)\n",
    "+ II. Setup\n",
    "+ III. TF-IDF with Scikit-Learn\n",
    "+ IV. Breaking down the TF-IDF formula\n",
    "+ V. Calculating TF-IDF\n",
    "+ VI. Automating the TF-IDF process with a function\n",
    "+ VII. TF-IDF Vectors with Ngrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Word Vectors\n",
    "\n",
    "**Text vectorization** is the process of converting texts into numbers or, more specifically, into vectors of numbers. \n",
    "\n",
    "[explain more]\n",
    "\n",
    "[what can we learn?]\n",
    "\n",
    "There are different methods of text vectorization. Three of the most common examples are:\n",
    "+ **Term Frequency - Inverse Document Frequency (TF-IDF)**:\n",
    "    + *Term frequency* is the number of times a word appears in one document. *Inverse document frequency** is - more or less - how frequently the word appears across the entire corpus in which this document is found. Thus, within a corpus of newspaper articles, an article on a baseball game will return high TF-IDF scores for words like \"hit\", \"run\", \"RBI\", and \"innings\" as well as the names of teams and individual players. But, common words found in that same article, like \"the\", \"this\", \"and\", etc. will have low TF-IDF scores. \n",
    "+ **Word2Vec**: \n",
    "    + Word2Vec is a method to convert a word to a numerical array that essentially situates the word into a multi-dimension language space where similar words are found close to one another. [more] [\"embeddings\"]\n",
    "    + applying this vectorization method to a corpus is significantly faster than the other two methods mentions here\n",
    "    + however, TF-IDF is a far simpler process to understand\n",
    "+ **Sentence-BERT**:\n",
    "    + Instead of creating a vector for each word, Sentence-BERT creates a vector for each sentence. This allows the encoding of a word's context: for example, that the *bow* of a ship is something altogether different from a *bow* that you tie or a *bow* and arrow.\n",
    "    + unlike Word2Vec and like TF-IDF, however, this method is computationally intensive and takes up a lot of memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II: Setup\n",
    "\n",
    "1. Before beginning, we need to import some packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "from pathlib import Path\n",
    "import glob \n",
    "import pandas as pd\n",
    "\n",
    "textdir = Path(\"~/shared/RR-workshop-data/state-of-the-union-dataset/txt\").expanduser() \n",
    "pathlist = sorted(textdir.glob('*.txt')) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Term Frequency - Inverse Data Frequency (TFIDF)\n",
    "\n",
    "<img src = \"https://miro.medium.com/max/720/1*qQgnyPLDIkUmeZKN2_ZWbQ.webp\" style=\"width:60%\">\n",
    "\n",
    "Image from Yassine Hamdaoui, [\"TF(Term Frequency)-IDF(Inverse Document Frequency) from scratch in python\"](https://towardsdatascience.com/tf-term-frequency-idf-inverse-document-frequency-from-scratch-in-python-6c2b61b78558) *Towards Data Science (Medium)* (Dec. 9, 2019).\n",
    "\n",
    "***Portions of this notebook are taken from the lesson [\"TF-IDF with Scikit-learn\"](https://melaniewalsh.github.io/Intro-Cultural-Analytics/05-Text-Analysis/03-TF-IDF-Scikit-Learn.html) in Melanie Walsh's Introduction to Cultural Analytics & Python book  (indicated by the [MW]).***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III. TF-IDF with Scikit-Learn [MW]\n",
    "\n",
    "Tf-idf is a method that tries to identify the most distinctively frequent or significant words in a document. \n",
    "\n",
    "In this lesson, we’re going to learn how to calculate tf-idf scores using a collection of plain text (.txt) files and the Python library scikit-learn, which has a quick and nifty module called TfidfVectorizer.\n",
    "\n",
    "In this lesson, we will cover how to:\n",
    "\n",
    "    Calculate and normalize tf-idf scores for U.S. Inaugural Addresses with scikit-learn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IV. Breaking Down the TF-IDF Formula [MW]\n",
    "\n",
    "But first, let’s quickly discuss the tf-idf formula. The idea is pretty simple.\n",
    "\n",
    "**tf-idf = term_frequency * inverse_document_frequency**\n",
    "\n",
    "**term_frequency** = number of times a given term appears in document\n",
    "\n",
    "**inverse_document_frequency** = log(total number of documents / number of documents with term) + 1*****\n",
    "\n",
    "You take the number of times a term occurs in a document (term frequency). Then you take the number of documents in which the same term occurs at least once divided by the total number of documents (document frequency), and you flip that fraction on its head (inverse document frequency). Then you multiply the two numbers together (term_frequency * inverse_document_frequency).\n",
    "\n",
    "The reason we take the inverse, or flipped fraction, of document frequency is to boost the rarer words that occur in relatively few documents. Think about the inverse document frequency for the word “said” vs the word “pigeon.” The term “said” appears in 13 (document frequency) of 24 (total documents) Lost in the City stories (24 / 13 –> a smaller inverse document frequency) while the term “pigeons” only occurs in 2 (document frequency) of the 24 stories (total documents) (24 / 2 –> a bigger inverse document frequency, a bigger tf-idf boost).\n",
    "\n",
    "*There are a bunch of slightly different ways that you can calculate inverse document frequency. The version of idf that we’re going to use is the scikit-learn default, which uses “smoothing” aka it adds a “1” to the numerator and denominator:\n",
    "\n",
    "**inverse_document_frequency** = log((1 + total_number_of_documents) / (number_of_documents_with_term +1)) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V. Calculate tf-idf [MW]\n",
    "\n",
    "To calculate tf–idf scores for every word, we’re going to use scikit-learn’s TfidfVectorizer.\n",
    "\n",
    "4. When you initialize TfidfVectorizer, you can choose to set it with different parameters. These parameters will change the way you calculate tf–idf.\n",
    "\n",
    "The recommended way to run TfidfVectorizer is with smoothing (smooth_idf = True) and normalization (norm='l2') turned on. These parameters will better account for differences in text length, and overall produce more meaningful tf–idf scores. Smoothing and L2 normalization are actually the default settings for TfidfVectorizer, so to turn them on, you don’t need to include any extra code at all.\n",
    "\n",
    "Initialize TfidfVectorizer with desired parameters (default smoothing and normalization).\n",
    "\n",
    "**Note: tfidf vectors can become very large even for a modest number of texts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(input='filename', stop_words='english')\n",
    "tfidf_vectorizer2 = TfidfVectorizer(input='filename', stop_words='english', max_df = 0.5, max_features=5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Run TfidfVectorizer on our text_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vector = tfidf_vectorizer.fit_transform(pathlist)\n",
    "tfidf_vector\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Make a DataFrame out of the resulting tf–idf vector, setting the “feature names” or words as columns and the titles as rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_titles = [path.stem for path in pathlist]\n",
    "#TfidfVectorizer returns a sparse matrix and that's why we have to call .toarray()  before proceeding.\n",
    "tfidf_df = pd.DataFrame(tfidf_vector.toarray(), index=text_titles, columns=tfidf_vectorizer.get_feature_names_out())\n",
    "#warning: get_feature_names will be depreciated; use get_feature_names_out instead\n",
    "   ##I made this fix in the code above\n",
    "print(tfidf_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_df.index.name = \"textname\"\n",
    "tfidf_df = tfidf_df.reset_index()\n",
    "tfidf_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_long =  pd.melt(tfidf_df, id_vars = \"textname\", var_name = \"word\", value_name = \"tfidf_score\", value_vars = list(tfidf_df.drop(columns = [\"textname\"]).columns))\n",
    "tfidf_long.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tfidf_long.shape)\n",
    "tfidf_long = tfidf_long[tfidf_long['tfidf_score'] > 0.0]\n",
    "print(tfidf_long.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get top 15 tfidf scores for each text\n",
    "N = 15\n",
    "tfidf_long = tfidf_long.sort_values(by = \"tfidf_score\", ascending=False)\n",
    "print(tfidf_long.shape)\n",
    "tfidf_sub = tfidf_long.groupby('textname').head(N).reset_index(drop=True)\n",
    "\n",
    "tfidf_sub.head(50)\n",
    "\n",
    "#textnames = list(tfidf_long['textname'].unique())\n",
    "\n",
    "#for i, text in enumerate(textnames):\n",
    "#    onetext_df = tfidf_sub[tfidf_sub['textname'] == text]\n",
    "#    print(onetext_df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" role=\"alert\" style=\"color:blue\">\n",
    "    <h3><b>Exercises</b>:</h3> \n",
    "    <p>7. Subset the tfidf_sub dataframe to examine the top tfidf_scores for one particular speech</p>\n",
    "    <p>7b Advanced. Subset the tfidf_sub dataframe to examine the top tfidf_scores for a president.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VI. Automate the TF-IDF vectorization process with a function\n",
    "\n",
    "8. The function below integrates some of the tasks we did above to further automate the process of creating tf-idf vectors. \n",
    "\n",
    "Examine the `tfidf_analysis` function below. What are its inputs? What does it return (its output)? Can you identify what each line of code does?\n",
    "\n",
    "You may notice, that this code applies an additional processing step to our text: it lemmatizes tokens from the text, reducing words to their base form (plural --> singular for nouns, present tense first-person for verbs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer   ###\n",
    "from nltk.corpus import stopwords\n",
    "stop = sorted(stopwords.words('english'))\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer(r'\\w+')   \n",
    "\n",
    "# Interface lemma tokenizer from nltk with sklearn\n",
    "class LemmaTokenizer:                                               ###\n",
    "    #ignore_tokens = [',', '.', ';', ':', '\"', '``', \"''\", '`']      ###\n",
    "    def __init__(self):                                             ###\n",
    "        self.wnl = WordNetLemmatizer()                              ###\n",
    "    def __call__(self, doc):                                        ###\n",
    "        #return [self.wnl.lemmatize(t) for t in word_tokenize(doc) if t not in self.ignore_tokens]\n",
    "        return [self.wnl.lemmatize(t) for t in tokenizer.tokenize(doc) if not t.isdigit()] # if t not in self.ignore_tokens]    ###\n",
    "    \n",
    "lemma_tokenizer = LemmaTokenizer()                                 ###\n",
    "eng_stops = set(stopwords.words('english'))                        ###\n",
    "lemma_stop = lemma_tokenizer(' '.join(eng_stops))                  ###\n",
    "\n",
    "def tfidf_analysis(textdir, ng_range = (1,1), lemmas = False):\n",
    "    '''\n",
    "    textdir = pathlib Path object to folder containing .txt files to be analyzed\n",
    "    ng_range = range of ngrams to be analyzed, i.e. (1,2) will analyze words of length 1 (unigrams) and 2 (bigrams) \n",
    "    reads in a file folder and returns a long tfidf dataframe for all .txt files found in this folder\n",
    "    Steps:\n",
    "    1. \n",
    "    '''\n",
    "    #tfidf_vectorizer = TfidfVectorizer(input='filename', stop_words='english', ngram_range = (ng_range))\n",
    "    if lemmas:\n",
    "        tfidf_vectorizer = TfidfVectorizer(input = \"filename\", stop_words = lemma_stop, tokenizer = lemma_tokenizer, ngram_range = (ng_range), max_df = 0.5, max_features=5000)  #$$$$\n",
    "    else:\n",
    "        tfidf_vectorizer = TfidfVectorizer(input = \"filename\", stop_words = \"english\", ngram_range = (ng_range), max_df = 0.5, max_features=5000)  #$$$$\n",
    "        \n",
    "    pathlist = sorted(textdir.glob('*.txt'))\n",
    "    tfidf_vector = tfidf_vectorizer.fit_transform(pathlist)\n",
    "    text_titles = [path.stem for path in pathlist]\n",
    "    tfidf_df = pd.DataFrame(tfidf_vector.toarray(), index=text_titles, columns=tfidf_vectorizer.get_feature_names_out())\n",
    "    print(\"df shape: \", tfidf_df.shape)\n",
    "    tfidf_df = tfidf_df.loc[: ,(tfidf_df.max(numeric_only = True) > 0.02)]\n",
    "    print(\"df shape: \", tfidf_df.shape)\n",
    "    #print(tfidf_df.head())\n",
    "    tfidf_df.index.name = \"textname\"\n",
    "    tfidf_df = tfidf_df.reset_index()\n",
    "    tfidf_long =  pd.melt(tfidf_df, id_vars = \"textname\", var_name = \"word\", value_name = \"tfidf_score\", value_vars = list(tfidf_df.drop(columns = [\"textname\"]).columns))\n",
    "    \n",
    "    tfidf_long = tfidf_long.sort_values(by = 'tfidf_score', ascending = False)\n",
    "    print(\"df shape: \", tfidf_long.shape)\n",
    "    #print(tfidf_long.head(10))\n",
    "    return(tfidf_long)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. We can call the function below. You can try it with or without lemmatization and with single words or n-grams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#take 4x longer to run with lemmatizing tokenizer!\n",
    "longdf = tfidf_analysis(textdir, ng_range = (1,1), lemmas = True)\n",
    "longdf.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. The above dataframe is large: it has 1.1 million rows. We can reduce it by just keeping the top *N* words by tfidf score for each president. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 10\n",
    "longdf_sub = longdf.groupby('textname').head(N).reset_index(drop=True)\n",
    "print(longdf_sub.shape)\n",
    "longdf_sub.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" role=\"alert\" style=\"color:blue\">\n",
    "    <h3><b>Exercises</b>:</h3> \n",
    "    <p>11. To view the dataframe differently we can sort it by first year and then tfidf_score. Do so below:</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "longdf_sub['year'] = longdf_sub['textname'].str[-4:].astype(int)\n",
    "longdf_sub = longdf_sub.sort_values(by = [\"year\", \"tfidf_score\"], ascending = [False, False])\n",
    "longdf_sub.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" role=\"alert\" style=\"color:blue\">\n",
    "    <h3><b>Exercises</b></h3>\n",
    "    <p>12. Subset the dataframe by year, keeping only those speeches given on or after the year 2000.<p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "longdf_21C = longdf_sub.loc[(longdf_sub['year'] >= 2000), :]\n",
    "longdf_21C = longdf_21C.sort_values(by = \"year\", ascending = False)\n",
    "longdf_21C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VII. TFIDF vectors with ngrams\n",
    "\n",
    "TF-IDF works just as well on multi-word terms or phrases (ngrams) as it does on individual words. Besides revealing insight into the content of a text, TF-IDF analysis of ngrams in a text corpus can also reveal individuals' stylistic idiosyncrasies. Are there any phrases that use frequently in your speech? In your writing? \n",
    "\n",
    "An example of TF-IDF analysis using Ngrams is found in **fivethirtyeight**'s [These are the Phrases each GOP Candidate Repeats Most](https://fivethirtyeight.com/features/these-are-the-phrases-each-gop-candidate-repeats-most/).\n",
    "\n",
    "13. With the function `tfidf_analysis` we can compile tfidf_scores for ngrams, including two-, three-, and four-word terms by adjusting the minimum and maximum ngram length in the tuple called by the parameter `ng_range`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 15\n",
    "min_ng = 2\n",
    "max_ng = 3\n",
    "ng_longdf = tfidf_analysis(textdir, ng_range = (min_ng, max_ng), lemmas = True)\n",
    "ng_longdf = ng_longdf.sort_values(by = \"tfidf_score\", ascending=False)\n",
    "ng_longdf_sub = ng_longdf.groupby('textname').head(N).reset_index(drop=True)\n",
    "print(ng_longdf_sub.shape)\n",
    "ng_longdf_sub.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ng_longdf_sub.to_csv(f\"sotu_{min_ng}-{max_ng}grams_tfidf_top{N}.csv\", encoding = 'utf-8')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
