{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Measuring the Similarity of Texts using TF-IDF\n",
    "\n",
    "This notebook is modeled on the *Programming Historian* lesson [Understanding and Using Common Similarity Measures for Text Analysis](https://programminghistorian.org/en/lessons/common-similarity-measures) by John Ladd. Please visit this webpage for more explanation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Setup\n",
    "\n",
    "### Ia. Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "from pathlib import Path\n",
    "import glob \n",
    "import pandas as pd, numpy as np\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "import nltk\n",
    "from nltk import RegexpTokenizer  \n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "from nltk.corpus import stopwords\n",
    "stop = sorted(stopwords.words('english'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ib. Read in text files and create a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "textdir = Path(\"~/shared/RR-workshop-data/state-of-the-union-dataset/txt\").expanduser() \n",
    "pathlist = sorted(textdir.glob('*.txt')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "#n=50\n",
    "\n",
    "txtList=[]\n",
    "pathlist = sorted(textdir.glob('*.txt'))      # .glob only stores the pathlist temporarily (for some reason), so you need to call it again!2\n",
    "for path in pathlist:\n",
    "    fn=path.stem                       #stem returns the filename minus the \".txt\" (file extension). \n",
    "    pres,year=fn.split(\"_\")            # fn = \"1794_Washington\" becomes year = \"1794\" and pres = \"Washington\"\n",
    "    with open(path,'r') as f:  \n",
    "        text1 = f.read()                #opens each file and reads it in as \"sotu\"\n",
    "    tokens=tokenizer.tokenize(text1)    # tokenizes \"sotu\"\n",
    "    numtoks = len(tokens)             # counts the number of tokens in \"sotu\"\n",
    "    ltokens_ns = [tok.lower() for tok in tokens if tok not in stop]\n",
    "    txtList.append([pres, year, numtoks, tokens, ltokens_ns, text1])   #add this info for \"sotu\" to a running list for all sotu addresses\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colnames=['pres','year','numtoks','tokens', 'ltoks_ns', 'fulltext']\n",
    "textdf=pd.DataFrame(txtList, columns=colnames)  #places our completed list of SOTU info in a dataframe\n",
    "textdf.head(10)                                #prints out the first 10 rows of this dataframe (the default value for head() is 5 rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "textdf.sort_values(by = \"year\", ascending = False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Create a TF-IDF matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk.stem import WordNetLemmatizer   ###\n",
    "\n",
    "# Interface lemma tokenizer from nltk with sklearn\n",
    "class LemmaTokenizer:                                               ###\n",
    "    ignore_tokens = [',', '.', ';', ':', '\"', '``', \"''\", '`']      ###\n",
    "    def __init__(self):                                             ###\n",
    "        self.wnl = WordNetLemmatizer()                              ###\n",
    "    def __call__(self, doc):                                        ###\n",
    "        #return [self.wnl.lemmatize(t) for t in word_tokenize(doc) if t not in self.ignore_tokens]\n",
    "        return [self.wnl.lemmatize(t) for t in tokenizer.tokenize(doc) if t not in self.ignore_tokens]    ###\n",
    "    \n",
    "lemma_tokenizer = LemmaTokenizer()                                 ###\n",
    "eng_stops = set(stopwords.words('english'))                        ###\n",
    "lemma_stop = lemma_tokenizer(' '.join(eng_stops))   \n",
    "tfidf_vectorizer3 = TfidfVectorizer(input = \"filename\", stop_words = lemma_stop, tokenizer = lemma_tokenizer)\n",
    "tfidf_matrix = tfidf_vectorizer3.fit_transform(pathlist)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
    "#print(cosine_sim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III. Measuring similarity\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_array = tfidf_matrix.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "textnamelist = [path.stem for path in pathlist]\n",
    "euclidean_distances = pd.DataFrame(squareform(pdist(tfidf_array)), index=textnamelist, columns=textnamelist)\n",
    "print(euclidean_distances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tgt = \"Lincoln_1862\"      #try plugging in the names of different SOTU addresses, to view possible choices, enter the following in a new code cell: `textnamelist`\n",
    "top5_euclidean = euclidean_distances.nsmallest(10, tgt)[tgt][1:]\n",
    "print(top5_euclidean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_distances = pd.DataFrame(squareform(pdist(tfidf_array, metric='cosine')), index=textnamelist, columns=textnamelist)\n",
    "\n",
    "top5_cosine = cosine_distances.nsmallest(6, tgt)[tgt][1:]\n",
    "print(top5_cosine)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
