{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text by the Numbers: Working with One Text\n",
    "\n",
    "**A Reproducible Research Workshop**\n",
    "\n",
    "(A Collaboration between Dartmouth Library and Research Computing)\n",
    "\n",
<<<<<<< HEAD
=======
    "Instructors: Jeremy Mikecz, Lora Leligdon, & Simon Stone\n",
    "\n",
    "Research Data Services, Dartmouth Library\n",
    "\n",
    "*to schedule a personal consultation with one of us, you can reach us at our personal emails or at*: `ResearchDataHelp@groups.dartmouth.edu`\n",
    "\n",
>>>>>>> session-02
    "[*Click here to view or register for our current list of workshops*](http://dartgo.org/RRADworkshops)\n",
    "\n",
    "*This notebook created by*:\n",
    "+ Version 1.0: Jeremy Mikecz, Research Data Services (Dartmouth Library)\n",
    "\n",
    "<!--\n",
    "+ Some of the inspiration for the code and information in this notebook was taken from https://www.w3schools.com/python/python_intro.asp -- This is a great resource if you want to learn more about Python!-->\n",
    "\n",
    "**How can we use computational techniques to analyze texts and then visualize patterns buried within them?** \n",
    "\n",
    "**What can we learn about texts by applying text analysis in Python? How do we get started?**\n",
    "\n",
<<<<<<< HEAD
    "This is **Notebook 1** of 2 for the **Text by the Numbers: Text Analysis Methods in Python** workshop:\n",
=======
    "This is **Notebook 1** of 3 for the **Text by the Numbers: Text Analysis Methods in Python** workshop:\n",
>>>>>>> session-02
    "+ **Notebook 1: Text by the Numbers: Working with one text**\n",
    "+ Notebook 2: Text by the Numbers 2: Working with a Corpus\n",
    "+ Notebook 3: Text by the Numbers 3: Word Vectors\n",
    "\n",
    "In this session, participants will:\n",
    "\n",
    "+ Learn how to write basic scripts in Python using Jupyter Notebooks\n",
    "+ import and pre-process documents (into lists of words or tokens, lower-case each word, remove \"stopwords\", etc.)\n",
    "+ analyze each document using word frequencies, collocations, ngram frequencies, etc.\n",
    "\n",
    "**Table of Contents**\n",
    "\n",
    "+ Part I: Navigating through the files in a directory (whether on your computer or on a remote server)\n",
    "+ Part II. Divide a text into tokens\n",
    "+ Part III: Counting Words\n",
    "+ Part IV: NLTK functions\n",
    "+ Part V: Ngrams (multi-word terms and phrases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part I: Navigating through the files in a directory (whether on your computer or on a remote server)\n",
    "\n",
    "1. We will use the **Path** function from the **pathlib** library to navigate through our files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Next, we need to use this Path function to set the directory where we will get our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "textdir = Path(\"~/shared/RR-workshop-data/state-of-the-union-dataset/txt\").expanduser() \n",
    "pathlist = list(Path.iterdir(textdir))\n",
    "print([path.name for path in pathlist])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Let's open one text from this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(Path(textdir,\"Bush_2002.txt\"), encoding='utf-8') as f:\n",
    "    text1 = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
<<<<<<< HEAD
    "4. What do the following blocks of code do? Run them and then share your answer."
=======
    "4. What do you think the following lines of code do? Run them to test your hypotheses and then share your answer."
>>>>>>> session-02
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(text1))\n",
    "print(text1[:50])\n",
    "print(text1[0:50])\n",
    "print(\"***\\n\")\n",
    "print(text1[50:100])\n",
    "print(\"***\\n\")\n",
    "print(text1[-50:])\n",
    "text1[-50:-1]          #print function not necessary for the final line in a cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" role=\"alert\" style=\"color:blue\"><h3>Exercise for Part I</h3>\n",
    "    \n",
<<<<<<< HEAD
    "<p>4b. Choose another text in this corpus (or another!) to analyze. Open, read, and save it into memory adapting from the code in Step 3.</p>\n",
=======
    "<p>4b. Choose another text in this corpus (or another!) to analyze. Open, read, and save it into memory adapting the code in Step 3.</p>\n",
>>>>>>> session-02
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part II. Divide a text into tokens\n",
    "\n",
    "Often the first step in computational text analysis is to **tokenize** a text into separate tokens. \"Tokens\" can be words, but also punctuation, sets of numbers, etc.\n",
    "\n",
    "5. There are different ways to tokenize a text. \n",
    "\n",
    "    a. The simplest way is to use the split function to split a text on each space: `text1.split()`. However, that leaves punctuation attached to adjacent words.\n",
    "\n",
    "    ```\n",
    "    tokens1 = text1.split()\n",
    "    ```\n",
    "    \n",
    "    b. The **Natural Language Toolkit (nltk)** package also provides two tools for tokenizing texts. The **word_tokenize()** method separates punctuation into different tokens. \n",
    "\n",
    "    ```\n",
    "    tokens2 = nltk.word_tokenize(text1)\n",
    "    ```\n",
    "\n",
    "    c. Finally, we can use NLTK's **RegexpTokenizer** to remove most punctuation (but, it keeps punctuation that is considered part of a word, such as the period in \"Mr.\" or \"Dr.\".) This is the method we use below.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import RegexpTokenizer  \n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "tokens = tokenizer.tokenize(text1)\n",
    "print(tokens[:30])\n",
    "print(len(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" role=\"alert\" style=\"color:blue\"><h3>Exercises for Part II</h3>\n",
    "    \n",
    "<p>6. Tokenize your chosen text using one of the above three methods.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III. Counting Words\n",
    "\n",
    "7. We can count the length of a text using the **len()** function. Run the following two lines of code below. What is the difference between the two?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(text1))\n",
    "print(len(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. We can create a **frequency list** of words (ahem... tokens) using the **Counter()** function from the Python **collections** library. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "tokfreqs = collections.Counter(tokens)\n",
    "tokfreqs.most_common(20)         #to view the 60 most common items in tokfreqs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. What do you notice? How can we make this frequency list more revealing?"
   ]
  },
  {
<<<<<<< HEAD
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
=======
   "cell_type": "markdown",
   "metadata": {},
>>>>>>> session-02
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**9. We can convert all the tokens to lower-case.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ltokens = [tok.lower() for tok in tokens]  \n",
    "print(ltokens[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**10. We often want to remove stopwords**. **Stop words** are common words that reveal little about the meaning of a text (such as articles like \"the\", conjunctions like \"and\", prepositions like \"on\", pronouns like \"our\", and helper verbs like \"can\"). Fortunately, NLTK provides a list of stop words in English (and other languages as well) that we can use to eliminate all such words from our texts.\n",
    "\n",
    "Let's examine stopwords in English:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "print(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11. What if you work with another language? Let's print out the language options for NLTK's stopwords:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# languages in nltk\n",
    "print(stopwords.fileids())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" role=\"alert\" style=\"color:blue\"><b>Exercises</b>\n",
    "    <p>12. Now try to print out stopwords from a language of your choice (using the same code we used above to print out English stopwords):</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "13. Returning to our English stopwords list, we can further modify our ltokens list by removing stopwords:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = stopwords.words('english')\n",
    "stop = sorted(stop)\n",
    "ltokens_ns = [tok for tok in ltokens if tok not in stop]        #list comprehension removes all stopwords from ltokens2\n",
    "\n",
    "print(\"We had\", len(ltokens), \"tokens in our ltoken2 list.\")\n",
    "print(\"beginning with:\", ltokens[:30], \" \\n\")\n",
    "\n",
    "print(\"After removing stop words, we now have\", len(ltokens_ns), \"tokens in our list.\")\n",
    "print(\"beginning with:\", ltokens_ns[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "14. We can then count the frequencies of words in this token list using the **Counter** function from the **collections** library we imported above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokfreqs = collections.Counter(ltokens_ns)\n",
    "tokfreqs.most_common()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "    <p style=\"color:blue\"><b>Exercises</b>:</p> \n",
    "    <p style=\"color:blue\">15. Using your chosen text:</p>\n",
    "    <ol style=\"color:blue\"> \n",
    "        <li>tokenize it</li> \n",
    "        <li>lower-case it</li> \n",
    "        <li>remove all stopwords</li>\n",
    "        <li>create a frequency count of the words in this address</li> \n",
    "        <li>identify the 30 most frequent tokens (with stopwords removed).</li>\n",
    "    </ol>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IV. Other NLTK functions\n",
    "\n",
    "The **Natural Language ToolKit (NLTK)** package provides many functions for analyzing texts. To learn more about NLTK, review the free, online [NLTK book](https://www.nltk.org/book/).\n",
    "\n",
    "16. To apply NLTK to our text, we need to import the tokenized list we created from it. Since it is not necessary to convert the text to lowercase or remove stopwords we will just use our list called `tokens`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk_text1 = nltk.Text(tokens) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "17. Our chosen text is now saved as `text1`. Try running the following NLTK functions to see what they do. \n",
    "\n",
    "What can you learn about a text using these functions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk_text1.concordance(\"government\")   ### experiment by entering other words in place of \"government\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk_text1.collocations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk_text1.count(\"terror\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk_text1.dispersion_plot([\"freedom\", \"God\", \"liberty\", \"war\"])   ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk_text1.similar(\"freedom\")  ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "    <p style=\"color:blue\"><b>Exercises</b>:</p> \n",
    "    <p style=\"color:blue\">17b. Using your chosen text:</p>\n",
    "    <ol style=\"color:blue\"> \n",
    "        <li>convert it into a nltk.Text object</li> \n",
    "        <li>analyze it using some of the methods shown above. What can you learn?</li>\n",
    "    </ol>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V. Ngrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we want to examine the frequency of multiple-word terms and phrases? \n",
    "\n",
    "Instead of splitting texts into words, we can split them into bigrams (two-word combinations), trigrams (three-word combinations), and other *n*grams (terms with *n* number of words).\n",
    "\n",
    "A great tool for examining the frequency of ngrams over time is [Google's Ngram Viewer.](https://books.google.com/ngrams/) Click on this link and try some different combinations.\n",
    "\n",
    "<!--<h1>\n",
    "    <img src = \"C:\\Users\\F0040RP\\Documents\\Website\\images\\textAnalysis\\google_ngrams_history_subfields.png\" style = \"width: 80%\">\n",
    "    <img src = \"https://books.google.com/ngrams/graph?content=cultural+history%2Cpolitical+history%2Cintellectual+history%2Cmicrohistory%2C+women%27s+history&year_start=1800&year_end=2019&corpus=en-2019&smoothing=3\" style=\"width:80%;\">\n",
    "</h1>-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "18. To create, count, and analyze ngrams from our own texts and corpora, we can use **NLTK**'s **ngrams** function, which reads in a list of tokens and the number of words we want per combination. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 2\n",
    "bigrams = list(nltk.ngrams([\"to\", \"be\", \",\", \"or\", \"not\", \"to\", \"be\", \",\", \"that\", \"is\", \"the\", \"question\", \".\"], n))\n",
    "print(bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collections.Counter(bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 2\n",
    "n_grams=list(nltk.ngrams(ltokens_ns,n))\n",
    "print(n_grams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collections.Counter(n_grams).most_common()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" role=\"alert\" style=\"color:blue\"><h3>Exercises for Part V</h3>\n",
    "    \n",
    "<p>19. Create ngrams from your chosen text. Try bigrams, trigrams, and even ngrams of length 4. Then identify the most common of these ngrams. Are these results useful to understanding this particular text?</p>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
<<<<<<< HEAD
=======
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Thusfar, you have learned how to \n",
    "1. navigate to a given text file in a directory\n",
    "2. open and read that text into Python\n",
    "3. search for and count the frequency of given words\n",
    "4. apply some basic functions from the NLTK library\n",
    "5. and divide the text into ngrams to examine the frequency of multi-word terms and phrases.\n",
    "\n",
    "What steps would you add to the above to be able to scale this up and apply these methods across a large corpus of texts?"
   ]
  },
  {
>>>>>>> session-02
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
