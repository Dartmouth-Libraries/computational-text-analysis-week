{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Playground\n",
    "\n",
    "With a small group, or on your own, choose a text dataset and bring into this Jupyter Notebook for analysis.\n",
    "\n",
    "Possible text datasets you can use include:\n",
    "\n",
    "+ **corpora of plain text files** (see the `\"~/shared/RR-workshop-data/text_corpora\"` folder from within JHUB). Some examples:\n",
    "    - lyrics of 100+ of Taylor Swift songs\\**\n",
    "    - 366 New York Times obituaries\\*\n",
    "    - a corpus of colonial South Asia literature (1850 - 1923)\\*\n",
    "    - a corpus of African American literature (1850 - 1923)\\*\n",
    "    - a corpus of 450 novels written between 1770 and 1923 in English, French, and German (150 novels in each language).\\*\\*\\*\n",
    "+ **datasets containing metadata and/or fulltexts** within one .csv, .tsv, or .txt file (see \"text_data\" folder). (from our JHUB: `\"~/shared/RR-workshop-data/text_datasets\"`) Examples:\n",
    "    - metadata from all New York Times articles published in April 2023 (includes metadata like title, author, date, as well as a short summary, and the first paragraph of each)\n",
    "    - metadata from all New York Times articles published in October 2023 (includes metadata like title, author, date, as well as a short summary, and the first paragraph of each)\n",
    "    - Tab-separated files from fact-checking sites *Politifact*  and *Snopes*.\n",
    "    - top posts on the Reddit page \"Am I the Asshole?\"\n",
    "    - A dataset of Trump tweets between 2009 and 2021\n",
    "+ text files or corpora you have that are ready for analysis\n",
    "\n",
    "\n",
    "\\* linked to on the [datasets page](https://melaniewalsh.github.io/Intro-Cultural-Analytics/00-Datasets/00-Datasets.html) in Melanie Walsh's *Cultural Analytics* book.\n",
    "\n",
    "\\*\\* kaggle.com\n",
    "\n",
    "\\*\\*\\* This corpus was created by Andrew Piper of the **.txtlab** at McGill University ([documentation for this dataset is available here](https://txtlab.org/2016/01/txtlab450-a-data-set-of-multilingual-novels-for-teaching-and-research/)). This corpus includes separate files for each text and a summary csv with metadata for each."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some things you can experiment with:\n",
    "1. Reading the data into a Python dataframe\n",
    "2. creating summary statistics of the dataset\n",
    "3. subset the dataset as needed\n",
    "3. perform new calculations from this dataset and store in a separate column\n",
    "3. subset, filter, sort, or split-apply-combine the dataset to examine specific patterns\n",
    "4. create basic data visualizations with matplotlib or seaborn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
