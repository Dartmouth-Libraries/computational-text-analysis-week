{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Analysis with Large Language Models\n",
    "\n",
    "As we have seen in the previous notebooks, Large Language Models are very powerful text processing engines. We can therefore prompt them to do tasks beyond text summarization for us.\n",
    "\n",
    "In this notebook, we will use an LLM to tackle the text analysis tasks we have previously implemented in [Session 2](../../../02-text-by-the-numbers/).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading our API key\n",
    "\n",
    "At this point you should have set up a file named `secrets.env` with your OpenAI API key. We will now use a lightweight Python package called `dotenv` to read in this file and set its contents as environment variables:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv(\"../secrets.env\")\n",
    "\n",
    "os.getenv(\n",
    "    \"OPENAI_API_KEY\"\n",
    ") is not None  # Do not print the key itself! We want to keep it secret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing a model\n",
    "\n",
    "Since we want to process a rather long text all at once, let's pick a model with a large context window:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo-16k\", temperature=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading a document\n",
    "\n",
    "Pick any document you like! To keep the code from getting too complex, you may want to pick a single text of less than 10,000 words.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "docs_dir = Path(\n",
    "    \"~/shared/RR-workshop-data/federalist-papers-dataset/split\"\n",
    ").expanduser()\n",
    "\n",
    "with open(docs_dir / \"federalist_1.txt\") as f:\n",
    "    doc = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "\n",
    "When prompting the LLM to tokenize the text, try to add an instruction that makes the output easy to process! For example, you could request the output to be a list of comma-separated values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenization_prompt = (\n",
    "    \"Tokenize the following text into a comma-separated list of words: \\n\\n\" + doc\n",
    ")\n",
    "tokenized = llm.predict(tokenization_prompt)\n",
    "tokenized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stopword removal\n",
    "\n",
    "For stop word removal, we can try several approaches: We could provide a list of stopwords to remove, or let the LLM figure it out entirely.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_removed = llm.predict(\n",
    "    \"Remove all stop words from the following tokenized text: \\n\\n\" + tokenized\n",
    ")\n",
    "stopwords_removed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmed = llm.predict(\n",
    "    \"Apply stemming to the following tokenized text: \\n\\n\" + stopwords_removed\n",
    ")\n",
    "stemmed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatized = llm.predict(\n",
    "    \"Apply lemmatization to the following tokenized text: \\n\\n\" + stopwords_removed\n",
    ")\n",
    "lemmatized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic modeling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = llm.predict(\n",
    "    \"List the 5 main topics discussed in the following text: \\n\\n\" + doc\n",
    ")\n",
    "print(topics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion\n",
    "\n",
    "While it was relatively little work to achieve these results above, we have no insights into the process _how_ the LLM arrived at this output. This makes reproducing and/or discussing and reasoning about the results much more difficult.\n",
    "\n",
    "However, we can also create workflows that combine conventional analysis techniques with the Large Language Model's ability to generate natural language.\n",
    "\n",
    "A great example for such a workflow is [BERTopic](https://maartengr.github.io/BERTopic/api/bertopic.html): This technique first extracts topics in a similar way to what we have seen in session 2, but then leverages a Large Language Model to find natural language representations for those topics. So instead of a collection of keywords that represent the topic, you end up with a more intelligble title close to what we saw above. In contrast to letting the LLM do the entire topic modeling, we can backtrack in the analysis and identify which words or passages support each topic.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table >\n",
    "<tbody>\n",
    "  <tr>\n",
    "    <td style=\"padding:0px;border-width:0px;vertical-align:center\">    \n",
    "    Created by Simon Stone for Dartmouth College Library under <a href=\"https://creativecommons.org/licenses/by/4.0/\">Creative Commons CC BY-NC 4.0 License</a>.<br>For questions, comments, or improvements, email <a href=\"mailto:researchdatahelp@groups.dartmouth.edu\">Research Data Services</a>.\n",
    "    </td>\n",
    "    <td style=\"padding:0 0 0 1em;border-width:0px;vertical-align:center\"><img alt=\"Creative Commons License\" src=\"https://i.creativecommons.org/l/by/4.0/88x31.png\"/></td>\n",
    "  </tr>\n",
    "</tbody>\n",
    "</table>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
