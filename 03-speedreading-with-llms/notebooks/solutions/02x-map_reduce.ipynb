{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading our API key\n",
    "\n",
    "At this point you should have set up a file named `secrets.env` with your OpenAI API key. We will now use a lightweight Python package called `dotenv` to read in this file and set its contents as environment variables:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv(\"../secrets.env\")\n",
    "\n",
    "os.getenv(\n",
    "    \"OPENAI_API_KEY\"\n",
    ") is not None  # Do not print the key itself! We want to keep it secret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading text documents\n",
    "\n",
    "If you are running this on Dartmouth's JupyterHub, the documents are already stored as individual text files for you as a dataset online. If you are running this anywhere else, [download and extract the dataset](https://git.dartmouth.edu/lib-digital-strategies/RDS/datasets/federalist-papers-dataset/-/archive/main/federalist-papers-dataset-main.zip) and change the path in the next cell accordingly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "\n",
    "docs_dir = Path(\n",
    "    \"~/shared/RR-workshop-data/federalist-papers-dataset/split\"\n",
    ").expanduser()\n",
    "\n",
    "doc_loader = DirectoryLoader(docs_dir, show_progress=True)\n",
    "docs = doc_loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Map-reduce\n",
    "\n",
    "Just like with Stuffing, _LangChain_ also supports Map-reduce with specialized objects that facilitate the process.\n",
    "\n",
    "We split the total map-reduce chain into a map chain and a reduce chain. These chains consist of a model and a prompt:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo-16k\")\n",
    "\n",
    "# Map\n",
    "map_template = \"Write a concise summary of the following: {docs}\"\n",
    "map_prompt = PromptTemplate.from_template(map_template)\n",
    "map_chain = LLMChain(llm=llm, prompt=map_prompt)\n",
    "\n",
    "# Reduce\n",
    "reduce_template = \"Distill the following summaries into a single summary of the main topics: {doc_summaries}\"\n",
    "reduce_prompt = PromptTemplate.from_template(reduce_template)\n",
    "reduce_chain = LLMChain(llm=llm, prompt=reduce_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is not guaranteed that the sum of all the outputs of the _map_ chain fit into the context window of the _reduce_ chain. We therefore may have to run _map-reduce_ iteratively. We could implement that manually, but _LangChain_ offers some convenient components to help with this.\n",
    "\n",
    "In addition to these LLm chains, we need two components:\n",
    "\n",
    "1. An object that handles the stuffing part preceding the _map_ step\n",
    "2. An object that handles the reduce step\n",
    "\n",
    "We can use _LangChain_'s `StuffDocumentsChain` to do the first task:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.combine_documents.stuff import StuffDocumentsChain\n",
    "\n",
    "# Takes a list of documents, combines them into a single string, and passes this to an LLMChain\n",
    "combine_documents_chain = StuffDocumentsChain(\n",
    "    llm_chain=reduce_chain,\n",
    "    document_variable_name=\"doc_summaries\",  # This is the name of the prompt variable to fill with the stuffed documents\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second part can be handled by a `ReduceDocumentsChain`. Since it may have to run multiple iterations, this object needs to have a way to combine the input documents, in case they do not fit into the context window as one long string. This is defined by the object we defined in the previous step:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import ReduceDocumentsChain\n",
    "\n",
    "# Combines and iteravely reduces the mapped documents\n",
    "reduce_documents_chain = ReduceDocumentsChain(\n",
    "    # This is how the documents get combined\n",
    "    combine_documents_chain=combine_documents_chain,\n",
    "    # This is the chain to use to deal with documents that do not fit into the context window\n",
    "    collapse_documents_chain=combine_documents_chain,\n",
    "    # The maximum number of tokens to group documents into.\n",
    "    token_max=2000,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can put it all together in a `MapReduceDocumentsChain`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import MapReduceDocumentsChain\n",
    "\n",
    "# Combining documents by mapping a chain over them, then combining results\n",
    "map_reduce_chain = MapReduceDocumentsChain(\n",
    "    # Map chain\n",
    "    llm_chain=map_chain,\n",
    "    # Reduce chain\n",
    "    reduce_documents_chain=reduce_documents_chain,\n",
    "    # The variable name in the llm_chain to put the documents in\n",
    "    document_variable_name=\"docs\",\n",
    "    # Return the results of the map steps in the output\n",
    "    return_intermediate_steps=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run this chain for the first 10 documents:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(map_reduce_chain.run(docs[:10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table >\n",
    "<tbody>\n",
    "  <tr>\n",
    "    <td style=\"padding:0px;border-width:0px;vertical-align:center\">    \n",
    "    Created by Simon Stone for Dartmouth College Library under <a href=\"https://creativecommons.org/licenses/by/4.0/\">Creative Commons CC BY-NC 4.0 License</a>.<br>For questions, comments, or improvements, email <a href=\"mailto:researchdatahelp@groups.dartmouth.edu\">Research Data Services</a>.\n",
    "    </td>\n",
    "    <td style=\"padding:0 0 0 1em;border-width:0px;vertical-align:center\"><img alt=\"Creative Commons License\" src=\"https://i.creativecommons.org/l/by/4.0/88x31.png\"/></td>\n",
    "  </tr>\n",
    "</tbody>\n",
    "</table>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
