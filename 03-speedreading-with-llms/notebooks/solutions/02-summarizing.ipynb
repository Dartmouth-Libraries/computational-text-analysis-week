{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summarizing texts\n",
    "\n",
    "Large Language Models are very good at processing large amounts of text and paying attention to the most important parts. We can use this ability to summarize texts.\n",
    "\n",
    "In this notebook, we will use an LLM to summarize the [Federalist Papers](https://en.wikipedia.org/wiki/The_Federalist_Papers).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading our API key\n",
    "\n",
    "At this point you should have set up a file named `secrets.env` with your OpenAI API key. We will now use a lightweight Python package called `dotenv` to read in this file and set its contents as environment variables:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv(\"../../secrets.env\")\n",
    "\n",
    "os.getenv(\n",
    "    \"OPENAI_API_KEY\"\n",
    ") is not None  # Do not print the key itself! We want to keep it secret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading text documents\n",
    "\n",
    "If you are running this on Dartmouth's JupyterHub, the documents are already stored as individual text files for you as a dataset online. If you are running this anywhere else, [download and extract the dataset](https://git.dartmouth.edu/lib-digital-strategies/RDS/datasets/federalist-papers-dataset/-/archive/main/federalist-papers-dataset-main.zip) and change the path in the next cell accordingly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "docs_dir = Path(\n",
    "    \"~/shared/RR-workshop-data/federalist-papers-dataset/split\"\n",
    ").expanduser()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could use regular Python to read each of the papers like so:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(docs_dir / \"federalist_1.txt\") as f:\n",
    "    doc = f.read()\n",
    "\n",
    "print(doc[:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, we can benefit from _LangChain_'s document loaders, which reads all the files in the directory with just a few lines of code:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import DirectoryLoader\n",
    "\n",
    "doc_loader = DirectoryLoader(docs_dir, show_progress=True)\n",
    "docs = doc_loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a summary\n",
    "\n",
    "### Stuffing\n",
    "\n",
    "The most straightforward way to ask a model to summarize a piece of text, is to send the entire text and ask the LLM to summarize it. This approach is called _stuffing_:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from openai.error import InvalidRequestError\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "\n",
    "try:\n",
    "    for doc in docs[:3]:\n",
    "        print(\"-\" * 30)\n",
    "        print(\n",
    "            llm.predict(\n",
    "                \"Summarize the following text in 200 words: \\n\" + doc.page_content\n",
    "            )\n",
    "        )\n",
    "except InvalidRequestError as e:\n",
    "    print(e._message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the model successfully summarizes the first document, but fails to summarize the second one.\n",
    "\n",
    "The reason for this is the length of the text: A Large Language Model cannot process arbitrarily long input strings. The maximum number of tokens (one token ~ 3/4 word) an LLM can take into account while producing the next output token is called the _context window_ (or \\_context_length). You can think of this as the \"attention span\" of the model.\n",
    "\n",
    "Checking [the official documentation](https://platform.openai.com/docs/models/gpt-3-5), we confirm that the context window for the base `gpt-3.5-turbo` model is 4096. However, there is a model called `'gpt-3.5-turbo-16k'` that offers four times the context window. So let's switch to that and try again:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo-16k\")\n",
    "\n",
    "for doc in docs[:3]:\n",
    "    print(\"-\" * 30)\n",
    "    print(\n",
    "        llm.predict(\"Summarize the following text in 200 words: \\n\" + doc.page_content)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using a chain\n",
    "\n",
    "Since summarizing documents is a popular use-case, _LangChain_ offers a pre-configured chain `StuffDocumentsChain`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.llm import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains.combine_documents.stuff import StuffDocumentsChain\n",
    "\n",
    "# Define prompt\n",
    "prompt_template = \"Write a concise summary of the following:{text}\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(prompt_template)\n",
    "\n",
    "# Define LLM chain\n",
    "llm = ChatOpenAI(temperature=0, model_name=\"gpt-3.5-turbo-16k\")\n",
    "llm_chain = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "# Define StuffDocumentsChain\n",
    "stuff_chain = StuffDocumentsChain(llm_chain=llm_chain, document_variable_name=\"text\")\n",
    "\n",
    "print(stuff_chain.run(docs[:1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convenience wrapper\n",
    "\n",
    "We can simplify this even more by using the convenient wrapper `load_summarize_chain()`, which comes with a predefined prompt suitable for this task:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.summarize import load_summarize_chain\n",
    "\n",
    "chain = load_summarize_chain(llm=llm, chain_type=\"stuff\")\n",
    "\n",
    "chain.run(docs[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Map-reduce\n",
    "\n",
    "Stuffing works great for shorter texts and models with a large context window. But what if we want to process longer documents, or a much larger number of documents?\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "The size of the context window appears to be a major area of concern for the big AI companies: OpenAI has recently released a [new version of GPT-4](https://platform.openai.com/docs/models/gpt-4-and-gpt-4-turbo) with a context window of 128,000 tokens, and [Anthropic's Claude](https://www.anthropic.com/product) can process 100,000 tokens. These language models can therefore process entire books in a single prompt. It may therefore seem obsolete to worry about the issue of a short context window, but there are still even some of the big names, like [Google's PaLM 2](https://developers.generativeai.google/products/palm), that only offer relatively small context windows (e.g., 8,000 tokens). In the open-source community, models generally tend to have even smaller context windows (e.g., [Llama 2](https://github.com/facebookresearch/llama/blob/main/MODEL_CARD.md) with only 4000 tokens). There are also effects of the context length on a model's performance that are under active investigation.\n",
    "\n",
    "Of course, all of this may change given the rapid development speed in the field. So watch this space!\n",
    "\n",
    "</div>\n",
    "\n",
    "One technique to consider in this case is _Map-reduce_. It consists of two steps:\n",
    "\n",
    "1. **Map**: Chunk the texts to summarize into smaller units (e.g., shorter documents, paragraphs) and summarize each chunk\n",
    "2. **Reduce**: Treat the summaries like new documents and summarize the summaries with the LLM\n",
    "\n",
    "Given what we have learned so far, we have all the tools we need to implement this technique. Here is what we need:\n",
    "\n",
    "1. A summarization chain that produces one summary for each document.\n",
    "2. A second summarization chain that produces a final summary from the intermediate summaries.\n",
    "\n",
    "Theoretically, we could even use the same summarization chain for each step, but it may be helpful keep them separate to provide more specific prompts to each of them.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "1. Implement the first chain (`map_chain`) using a basic `LLMChain`. You can use the following prompt:\n",
    "\n",
    "```\n",
    "\"Write a concise summary of the following document: {text}\"\n",
    "```\n",
    "\n",
    "2. Implement the second chain (`reduce_chain`) using another `LLMChain`. Here, you can use the following prompt:\n",
    "\n",
    "```\n",
    "\"Create a consolidated summary based on the following summaries of individual letters from the Federalist Papers: {summaries}\"\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"Write a concise summary of the following document: {text}\"\n",
    "prompt = PromptTemplate.from_template(prompt_template)\n",
    "\n",
    "llm = ChatOpenAI(temperature=0, model_name=\"gpt-3.5-turbo-16k\")\n",
    "map_chain = LLMChain(llm=llm, prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"Create a consolidated summary based on the following summaries of individual letters from the Federalist Papers: {summaries}\"\n",
    "prompt = PromptTemplate.from_template(prompt_template)\n",
    "reduce_chain = LLMChain(llm=llm, prompt=prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can put it all together by iterating over all our documents, stuffing all summaries into a single string, and producing the intermediate summaries with the `map_chain`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create intermediate summaries\n",
    "summaries = []\n",
    "for doc in docs[:2]:\n",
    "    summary = map_chain.run(doc)\n",
    "    summaries.append(summary)\n",
    "\n",
    "# Stuff all summaries into the context\n",
    "stuffed_summaries = \"\\n---\\n\".join(summaries)\n",
    "print(stuffed_summaries)\n",
    "\n",
    "# Create final summary\n",
    "summary = reduce_chain.run(stuffed_summaries)\n",
    "print(\"********** Final summary **********\")\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad! And definitely faster than if we had produced it manually. So there you have it: Speedreading with LLMs!\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "Our manual implementation works just fine in this case, because all of the intermediate summaries combined fit into the context window of the model used in the _reduce_ step. This is not generally the case, though. If the sum of the intermediate summaries is still too long, we can use an iterative approach: For example, we can reduce just a few of the intermediate summaries at a time, and then reduce those summarized summaries further. We can repeat this process as many times as needed.\n",
    "\n",
    "_LangChain_ offers a few convenient objects to implement this behavior. This will take you deeper into the weeds of the framework, though, so it is considered beyond the scope of this workshop. If you want to take a peek at what that might look like, though, check out [this notebook](./02x-map_reduce.ipynb) as a jumping-off point.\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our [next notebook](./03-text-analysis-with-llms.ipynb), we will step away from text summarization and explore, how we can perform many of the analyses we have introduced in Session 2 using a Large Language Model!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table >\n",
    "<tbody>\n",
    "  <tr>\n",
    "    <td style=\"padding:0px;border-width:0px;vertical-align:center\">    \n",
    "    Created by Simon Stone for Dartmouth College Library under <a href=\"https://creativecommons.org/licenses/by/4.0/\">Creative Commons CC BY-NC 4.0 License</a>.<br>For questions, comments, or improvements, email <a href=\"mailto:researchdatahelp@groups.dartmouth.edu\">Research Data Services</a>.\n",
    "    </td>\n",
    "    <td style=\"padding:0 0 0 1em;border-width:0px;vertical-align:center\"><img alt=\"Creative Commons License\" src=\"https://i.creativecommons.org/l/by/4.0/88x31.png\"/></td>\n",
    "  </tr>\n",
    "</tbody>\n",
    "</table>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
